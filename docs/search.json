[{"path":"/articles/web_only/0_preprocessing.html","id":"ethics","dir":"Articles > Web_only","previous_headings":"","what":"Ethics","title":"0. Experimental Protocol and Preprocessing","text":"submitted research plan ethics review 21-01-21 (dd-mm-yy) received approval 24-01-21. protect personal data, smudged (private) email addresses document. submitted amendment plan 29-04-21, adding collect EEG data change meeting method -line -person laboratory visit. EEG experiments conducted 08-06-21 03-09-21. recruited participants ran EEG experiments, using facilities School Psychology, research plans submitted one co-authors affiliated .  stored raw data sub-folder, tests/inst/extdata project folder, pedxing. data first-level data folder R binary, collating raw data together, except EEG. Specifically, former includes EEG data, size limit R package. organising similar directory structure, one able replicate analysis steps. make available raw data repository hosted zenodo. grateful OpenAIRE project provides service free charge. Please cite work, use data, code / ideas. provide also free, using GNU GPLv3 license.","code":""},{"path":"/articles/web_only/0_preprocessing.html","id":"participants-and-data","dir":"Articles > Web_only","previous_headings":"","what":"Participants and Data","title":"0. Experimental Protocol and Preprocessing","text":"following code chunk logged main R packages used analysis. Sometimes, call functions within particular package using symbol, :: :::, used frequently. behaviour experiment, recruited 24 participants, using MS Team. hosted 24 -line meetings 25-Feb-21 20-Apr-21. Four participants dropped finished task. instance, participant, HLBQ, dropped know unzip task file 20 minutes. participants fill age handedness information questionnaire distributed. found missing data starting data analyses, protocol requiring analysis must start recruitment. analyse two pilot participants review research plan, including experimental procedure analysis plan. NA’s following code reflected missing data. mkKT show appointment time. Note recruited participants first experiment UK residents strict lock-order, procedure accommodate . contributes result exclude good number participants. table detailed reasons must exclude .","code":"pkg <- c(\"ggplot2\", \"data.table\") sapply(pkg, require, character.only = TRUE) wk <- ifelse(.Platform$OS.type == \"windows\",               shortPathName(\"F:/Documents/pedxing/\"),                \"/media/yslin/Tui/projects/pedxing/\") setwd(wk) rm(list = ls()) sNme_exp <- data.frame(sq = 1:24,                    s = c(\"P4jJ\", \"mNoy\", \"Dndk\", \"Bhix\", \"69WA\", \"wLu7\",                          \"B0kn\", \"HLBQ\", \"wXWs\", \"QU2L\",\"52ho\", \"wToD\",                          \"YSlV\", \"VFFt\", \"amug\", \"qlNp\", \"LG2o\", \"OcQN\",                          \"Jv1V\", \"zFpL\", \"PjiE\", \"5NNB\", \"OEWy\", \"mkKt\"),                    sex = c(\"female\", \"male\", \"male\", \"female\", \"male\",                            \"female\", \"male\", \"male\", \"female\", \"male\",                            \"male\", \"female\", \"female\", \"male\", \"female\",                            \"female\", \"female\", \"female\", \"female\", \"male\",                            \"female\",\"male\", \"female\", NA),                    age = c(22, 41, 47, 38, 27,  48,                            28, 22, 45, NA, 44, 40,                            44, 26, 51, 48, 32, 22,                            37, 64, 40, 38, 51, NA),                    handedness = c('r', 'r', 'r', 'r', 'r', 'l',                                   'r', 'r', 'r', NA, 'r', 'r',                                   'r', 'r', 'r', 'r', 'r', 'l',                                   'r', 'r', 'r', 'r', 'r', NA)) dropout_exp <- c(1, 8, 23, 24) discard_exp <- c(3, 5, 6, 10, 16) valid_exp <- c(2, 4, 7, 9, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22) sNme_exp[dropout_exp, ] sNme_exp[discard_exp, ] sNme_exp[valid_exp, ]  length(discard_exp) / (nrow(sNme_exp) - length(dropout_exp)) # 0.25"},{"path":"/articles/web_only/0_preprocessing.html","id":"on-line-experiments","dir":"Articles > Web_only","previous_headings":"Participants and Data","what":"On-line Experiments","title":"0. Experimental Protocol and Preprocessing","text":"end, drop 4 participants finished even started task, discard 5 participants (5 20). resulted discard rate 25%. rate , although high, unexpected considering circumstance using -line meeting recruit participants. includes one participant , interviewed practice trials, insisted decided cross road car passed, strategy instruction encourage. excluded reason, considering following task instruction. instruction encouraged participants make pre-car responses possible. Note contradict research aim recruited participants decide responses / going make, prior viewing imperative stimulus. Importantly, randomly presented four TTA conditions timing TTAs trial moved slightly adding number drawn \\(Unif(-0.1, 0.1)\\). two tables logged detailed reasons discard participants. logged information immediately meetings. following tables analysed participants behavioural study.","code":""},{"path":"/articles/web_only/0_preprocessing.html","id":"eeg-experiments","dir":"Articles > Web_only","previous_headings":"Participants and Data","what":"EEG Experiments","title":"0. Experimental Protocol and Preprocessing","text":"EEG data set, discard rate 12.5%, slightly lower behavioural data set. data set acquired lock-lift. still time people generally wary disease transmission. followed strict experimental protocol taking account circumstance. Specifically, used cleaning procedure pre- post-experiment gowned PPE. implemented procedure close NHS possibly . also restricted time running experiment minimise contact time, .e., limiting amount time install EEG electrodes.  circumstance, discard three participants attain good quality EEG signals set time limit. Although one might improve efficiency operating full PPE, must take certain amount practice. almost equal male female participants. numbers participant age groups, 20-30, 30-40, … 60-70, distribute largest number first group, gradually decrease. participants age groups 64 years old. can state data represent just student/academic group. grateful participants willing take part difficult time, especially upper age groups might risk life. used loop aggregate data together RData binary, named ped.rda. latter steps, repeatedly use binary file. following format data enable RStan software process. Lastly, showed two abnormal responses investigated reason behind appeared. concludes preprocessing step. Next load pedxing package, automatically bring ped.rda.","code":"sNme_eeg <- data.frame(sq = 1:24,                        s = c('pG2u','snv0','rnd0','NEd0','HBfP','xuzS',                               'EtJa','md0M','2uqO','4WWn','pdLD','lDHY',                              '3maD','5TtL','7pve','12UD','w3ug','bcKj',                              '55tV','I8BJ','T4Vy','JNvZ','ulp9','dDjo'),                        sex = c(\"female\", 'female', 'male', 'male', 'female','male',                                'female', 'male', 'female', 'male', 'female','male',                                'male', 'male', 'female', 'female', 'female','male',                                'female', 'female', 'female', 'female', 'male', 'male'),                        age = c(45, 22, 42, 29, 24, 60,                                61, 19, 53, 19, 35, NA,                                47, 28, 26, 28, 38, 19,                                38, 21, 19, 62, 22, 41),                        handedness = c('r', 'r', 'r', 'r', 'r', 'r',                                       'r', 'r', 'r', 'r', 'r', 'r',                                       'r', 'r', 'r', 'r', 'r', 'r',                                       'r', 'r', 'r', 'r', 'r', 'r'))  discard_eeg <- c(1, 8, 9) valid_eeg <- sNme_eeg$sq[-discard_eeg]  3/ nrow(sNme_eeg) ## 0.125 dtmp0 <- sNme_exp[valid_exp, ] dtmp1 <- sNme_eeg[valid_eeg, ] dtmp0$exp <- 'exp' dtmp1$exp <- 'eeg' d <- data.table(rbind(dtmp0, dtmp1))  dage <- as_tibble(d) dage$age_gp <- cut(dage$age, breaks = c(20, 30, 40, 50, 60, 70), right = TRUE) table(dage$age_gp) # (20,30] (30,40] (40,50] (50,60] (60,70]  #     11       9       7       2       3   range(d$age, na.rm = TRUE)  # 19 64  table(d$sex) # female   male  #    20     16 E <- c('exp', 'eeg') path <- c(\"tests/inst/extdata/exp/valid/\", \"tests/inst/extdata/eeg/valid/\")  ns0 <- length(list.files(path[1])); ns0 ns1 <- length(list.files(path[2])); ns1  fn <- \"Commotions_Output_1.csv\" sNme <- d$s; sNme ## Participation sequence ##  [1] \"mNoy\" \"Bhix\" \"B0kn\" \"wXWs\" \"52ho\" \"WtoD\" \"YSIV\" \"VFFt\" \"amug\" \"LG2o\" ## [11] \"OcQN\" \"Jv1V\" \"zFpL\" \"PjiE\" \"5NNB\" ## ## Alphanumeric order ##  [1] \"52ho\" \"5NNB\" \"amug\" \"B0kn\" \"Bhix\" \"Jv1V\" \"LG2o\" \"mNoy\" \"OcQN\" \"PjiE\" ## [11] \"VFFt\" \"wToD\" \"wXWs\" \"YSlV\" \"zFpL\"  tmpNme <- c(\"Participant\", \"Trial\", \"TTA\", \"Jitter\", \"Side\", \"signalDelay\",             \"TrialStartTime\", \"CarAppearingTime\", \"CrossingkeyPressedTime\",              \"Hit\", \"OtherKeyPressedTime\", \"FirstKeyPressed\", \"AvgFPS\") d0 <- NULL for(i in seq_len(2)) {     path <- paste0('tests/inst/extdata/', E[i], '/valid/')         sNme <- d[exp == E[i]]$s     ns <- length(sNme)          for(j in seq_len(ns)) {         dfile <- paste0(path, sNme[j],\"/\",fn); dfile                  # This prints a figure to check over whether Unity results in correct          # delay time jitter.         output <- paste0(path, sNme[j], '/result_bk.ps')          tmp <- default_result(fn = fn, path = paste0(path, sNme[j]),                               output = output, verbose = FALSE)                      dtmp <- fread(dfile)         names(dtmp) <- tmpNme              dtmp$s <- d[s == sNme[j]]$sq         dtmp$age <- d[s == sNme[j]]$age         dtmp$sex <- d[s == sNme[j]]$sex                  if(!d[s == sNme[j]]$s == sNme[j]) stop(\"Subject name mismatched\")                  dtmp$sNme <- sNme[j]         dtmp$exp <- E[i]                  d0 <- rbind(d0, dtmp[, 2:18])     } }      ## d0 names(d0) <- c(\"Trial\", \"TTA\", \"Jitter\",  \"Side\", \"Delay\", \"TrialStartTime\",    \"CarAppearingTime\", \"CrossingkeyPressedTime\", \"Hit\", \"OtherKeyPressedTime\",   \"FirstKeyPressed\", \"AvgFPS\",  \"s\", \"age\", \"sex\", \"sNme\", \"E\")  #### Rename columns --------------------------- d0$BeforeCarPassed <- (d0$CrossingkeyPressedTime - d0$CarAppearingTim) < (d0$TTA + d0$Jitter) d0$RT <- d0$CrossingkeyPressedTime - d0$CarAppearingTim d0$TTANme <- factor(d0$TTA) d0$Side <- factor(d0$Side)  # These two lines show a possible timing imprecision in Unity software # d0$RT0 <- d0$CrossingkeyPressedTime - (d0$TrialStartTime + d0$signalDelay) # d0$RT1 <- d0$CrossingkeyPressedTime - (d0$CarAppearingTim)  d0$R <- factor(ifelse(d0$Hit, \"hit\", \"safe\")) d0$C <- as.logical(ifelse(d0$Hit, 0, 1)) # levels(d0$TTANme)  d0$G <- factor(ifelse(d0$BeforeCarPassed, \"before\", \"after\"))  dat <- d0[, c('Trial', 'Jitter',\"BeforeCarPassed\", \"E\", 'TTA', 'Side','R', 'C',               'G', 'TTANme', 'RT', 's')]  dat[E == \"exp\", .N, .(s)] dat[E == \"exp\", .N, .(s, TTA)]  dat[E == \"eeg\", .N, .(s)] dat[E == \"eeg\", .N, .(s, TTA)]  ## Add D and DTTA columns for legacy code dat$D <- 40  dat$Side <- factor(dat$Side) dat$DTTA <- factor(paste0(dat$D, \"-\", dat$TTA),                     levels = c('40-2.5', '40-3', '40-3.5', '40-4'))  dat[E == \"exp\", .N, .(TTA, s)] dat[E == \"eeg\", .N, .(TTA, s)] valid_exp <- c(2, 4, 7, 9, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22) valid_eeg <- c(2:7, 10:24) dexp <- dat[E == \"exp\" & s %in% valid_exp] deeg <- dat[E == \"eeg\" & s %in% valid_eeg]  dexp$s <- factor(dexp$s, levels = valid_exp) deeg$s <- factor(deeg$s, levels = valid_eeg) dexp$snew <- paste0(dexp$s, dexp$E) deeg$snew <- paste0(deeg$s, deeg$E)  d <- rbind(dexp, deeg) d$TTANme <- factor(d$TTANme, levels = c(2.5, 3, 3.5, 4))   d$sold <- d$s d$s <- as.integer( factor(d$snew) ) setorder(d, s, TTA, Side, G, R)  d$TTAint <- as.integer( as.integer(factor(d$TTA)) - 1 ) d$side   <- as.integer( as.integer(d$Side) - 1 ) d$TTAint <- as.integer( as.integer(factor(d$TTA)) - 1 ) d$side   <- as.integer( as.integer(d$Side) - 1 )  ## dat reduces columns ## d0 has all the columns save(d0, dat, d, file = \"data/ped.rda\") # tools::resaveRdaFiles(\"data/ped.rda\", compress = \"xz\", #                          compression_level = 9) # see R documentation \"ped\" for description of the dataset #### Two abnormal responses ------------------------------------------- ## Road width: 4.2m ## Initial distance from Kerb: 0.5 m ## Car Length: 4.96 m ## Car Width: 1.9 m ## Car initial distance: 40 m dtmp0 <- d0[d0$sNme == \"Bhix\" & d0$Trial == 28 & d0$TTA == 2.5,] dtmp1 <- d0[d0$sNme == \"VFFt\" & d0$Trial == 17 & d0$TTA == 2.5 & d0$Delay == 3, ] dtmp2 <- d0[d0$sNme == \"Bhix\" & d0$Trial == 27 & d0$TTA == 2.5,] dtmp3 <- d0[d0$sNme == \"Bhix\" & d0$Trial == 24 & d0$TTA == 2.5,]  time2pass <- (4.2+.5) / 1.6   car2pass <- 4.96 / 40 dtmp0$CarAppearingTime + dtmp0$TTA + dtmp0$Jitter dtmp0$CrossingkeyPressedTime   dtmp1$CarAppearingTime + dtmp1$TTA + dtmp1$Jitter dtmp1$CrossingkeyPressedTime   dtmp2$CarAppearingTime + dtmp2$TTA + dtmp2$Jitter dtmp2$CrossingkeyPressedTime   dtmp3$CarAppearingTime + dtmp3$TTA + dtmp3$Jitter dtmp3$CrossingkeyPressedTime   ## Conclusion: According to the Unity programmer, Unity  ## represents the pedestrian and car by using cylinders. Therefore, imprecision  ## representation might be the reason causing these two abnormal responses."},{"path":"/articles/web_only/1_glm.html","id":"model-based-analysis-of-manifest-responses","dir":"Articles > Web_only","previous_headings":"","what":"Model-based Analysis of Manifest Responses","title":"1. Multi-level Regression Models","text":"used logistic regression models separately analyse six key dependent variables: proportion responses made car passes given resulted collisions, proportion responses resulting safe crosses given made car passes, proportion responses resulting safe crosses given made car passes, Safe response times (RTs) responses made car passes, Safe RTs responses made car passes, Collision RTs made car passes separately analyse collision RTs car passes, total , respectively, 2, 0, 3, 1 trials four TTA conditions. loaded pre-processed data selected key variables. simple checks total number trial see whether design intends . Specifically, 256 trials, used (1) 2 levels LR factor, 4 levels TTA factor, tested 4 blocks participant, repeated 8 times unique set treatment combinations. checked number collision response condition. depends participants conditions.","code":"pkg <- c('data.table', 'ggplot2', 'rstan', 'rethinking', 'pedxing', 'dplyr') sapply(pkg, require, character.only = TRUE) rm (list = ls()) wk <- ifelse(.Platform$OS.type == \"windows\",               shortPathName(\"E:/Documents/pedxing/\"),               '/media/yslin/Tui/projects/pedxing/') setwd(wk) options(digits = 4) data(ped) cols <- c(\"TTA\",\"Side\",\"R\",\"G\",\"TTANme\",\"RT\",\"s\",\"sold\", \"E\")  d[,..cols]  table(d$s) #   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  # 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256  #  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  # 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256   ## ditto; should be all 64 d[, .N, .(E, TTA, s)]  ## should be 64 unique(d[, .N, .(E, TTA, s)]$N) dhit <- d[R==\"hit\"] %>% count(TTA, G, .drop = FALSE);  tibble::as_tibble(dhit) #    TTA      G   n # 1: 2.5  after   2 # 2: 2.5 before 752 # 3: 3.0  after   0 # 4: 3.0 before 213 # 5: 3.5  after   3 # 6: 3.5 before  17 # 7: 4.0  after   1 # 8: 4.0 before   6   prohit <- d[R == \"hit\", .(count = .N,                            MRT = 1000*mean(RT)), .(TTA, Side, G, s)] prohit[TTA == 2.5 & G == \"before\"] prohit[, total := sum(count), .(TTA, Side, s)] prohit[, value := count/total] setorder(prohit, TTA, G, Side, s) tibble::as_tibble(prohit[TTA == 2.5 & G == \"before\"]) # A tibble: 70 × 8 #      TTA Side  G          s count   MRT total value #    <dbl> <fct> <fct>  <int> <int> <dbl> <int> <dbl> #  1   2.5 left  before     1     8  591.     8     1 #  2   2.5 left  before     2    18  556.    18     1 #  3   2.5 left  before     3     7  793.     7     1 #  4   2.5 left  before     4     3  622.     3     1 # ... # … with 60 more rows"},{"path":"/articles/web_only/1_glm.html","id":"response-proportion","dir":"Articles > Web_only","previous_headings":"Model-based Analysis of Manifest Responses","what":"Response Proportion","title":"1. Multi-level Regression Models","text":"proportion responses made car passes, especially 3-s TTA, indicative participants mostly followed task instruction. Specifically, recorded three safe responses condition 2.5-s TTA 9216 trials. Within three responses, two show fast response times, can safely consider anticipated responses. speaks effectiveness adding small random time variation [sampled \\(U(-0.1, 0.1)\\)] TTA. one, namely 821.9 ms, falls upper 10% percentile collision responses 2.5-TTA condition. Therefore far, found clear explanation safe response, correctWe Thus, models optimising around, least, dataset, generate nearly pure model predictions, reflect model assumptions safe responses 2.5 TTA condition. prepare data format suitable RStan. following lists multiple logistic regression models. One able replicate modelling replacing data instance changing data.frame(dp_be) data.frame(dp_af). use population-level Gaussian distribution \\(location = 0\\), \\(scale =10\\), intercept parameter, model random-effect individual factor another Gaussian, parameter values, slope parameter. Depending models, one three slope hyper-parameters.","code":"pro <- d[R == \"safe\", .(count = .N,                         MRT = mean(RT)), .(TTA, Side, G, s)] names(d) cols <- c(\"Jitter\", \"RT\", \"G\", \"Side\", \"TTA\") d[TTA == 2.5 & G == \"before\" & R == \"safe\", ..cols] #     Jitter     RT      G  Side TTA # 1: 0.08506 0.8219 before  left 2.5 # 2: 0.01145 0.1370 before right 2.5 # 3: 0.01056 0.0400 before  left 2.5  quantile( d[TTA == 2.5 & G == \"before\" & R == \"hit\", ..cols]$RT,            probs = seq(0, 1, .05))[15:20] #    70%    75%    80%    85%    90%    95%   100%  # 0.6574 0.6800 0.7168 0.7580 0.8029 0.8984 2.3650  #     pro[TTA == 2.5 & G == \"before\"] #    TTA  Side      G  s count   MRT # 1: 2.5  left before 13     1 821.9 # 2: 2.5  left before 18     1  40.0 # 3: 2.5 right before 14     1 137.0 ns <- length(unique(d$s)); ns pro <- d[R == \"safe\", .(count = .N), .(TTAint, TTANme, side, G, s)] pro[TTAint == 0 & G == \"before\"] pro[, total := sum(count), .(TTAint, side, s)] pro[, value := count/total] dp_be <- pro[G == \"before\"] ## pre-car  percentage dp_af <- pro[G == \"after\"]   pro <- d[G == \"before\", .(count = .N), .(TTAint, TTANme, side, R, s)] pro[, total := sum(count), .(TTAint, side, s)] pro[, value := count/total] dsafe_be <- pro[R == \"safe\"]  ## replace with data.frame(dsafe_af) to get safe proportion | after the car pro <- d[G == \"after\", .(count = .N), .(TTAint, TTANme, side, R, s)] pro[, total := sum(count), .(TTAint, side, s)] pro[, value := count/total] dsafe_af <- pro[R == \"safe\"] m0 <- map2stan(   alist (     count ~ dbinom(total, p),     logit(p) <- a[s],     a[s] ~ dnorm(0, 10)   ),   data = data.frame(dp_be), chain = 4, iter = 3000, warmup = 1500, cores = 4)  m1 <- map2stan(   alist (     count ~ dbinom(total, p),     logit(p) <- a[s] + b1 * TTAint,     a[s] ~ dnorm(0, 10),     b1 ~ dnorm(0, 10)   ),   data = data.frame(dp_be), iter = 3000, warmup = 1500, chain = 4, cores = 4)  m2 <- map2stan(   alist (     count ~ dbinom(total, p),     logit(p) <- a[s] + b2 * side,     a[s] ~ dnorm(0, 10),     b2 ~ dnorm(0, 10)   ),   data = data.frame(dp_be), iter = 3000, warmup = 1500, chain = 4, cores = 4)  m3 <- map2stan(   alist (     count ~ dbinom(total, p),     logit(p) <- a[s] + b1 * TTAint +  b2 * side,     a[s] ~ dnorm(0, 10),     b1 ~ dnorm(0, 10),     b2 ~ dnorm(0, 10)   ),   data = data.frame(dp_be), iter = 3000, warmup = 1500, chain = 4, cores = 4)  m4 <- map2stan(   alist (     count ~ dbinom(total, p),     logit(p) <- a[s] + b1 * TTAint +  b2 * side + b3 * TTAint * side,     a[s] ~ dnorm(0, 10),     b1 ~ dnorm(0, 10),     b2 ~ dnorm(0, 10),     b3 ~ dnorm(0, 10)   ),   data = data.frame(dp_be), iter = 3000, warmup = 1500, chain = 4, cores = 4)  # save(m0, m1, m2, m3, m4, file = \"tests/2_preprocess/data/logistic_precar.RData\") # save(m0, m1, m2, m3, m4, file = \"tests/2_preprocess/data/logistic_safe_before.RData\") # save(m0, m1, m2, m3, m4, file = \"tests/2_preprocess/data/logistic_safe_after.RData\")"},{"path":"/articles/web_only/1_glm.html","id":"response-times","dir":"Articles > Web_only","previous_headings":"Model-based Analysis of Manifest Responses","what":"Response Times","title":"1. Multi-level Regression Models","text":"following code almost identical . replace logistic regression Gaussian (linear) regression models. Similar logistic model, also use population-level Gaussian distribution \\(location = 0\\), \\(scale =10\\), intercept parameter, model random-effect individual factor another Gaussian, parameter values, slope parameter. Depending models, one three slope hyper-parameters. use population-level Cauchy distribution, \\(location = 0\\), \\(scale = 2\\), model hyper-level variability parameters. Model 0 (m0), baseline model, assumes factor resulting different response proportions individual differences, governed population distribution \\(N(0, 10)\\). draws population distribution linked proportion parameter binomial distribution logit function. Model 1 Model 4 gradually add influences TTA R-L factors, well interaction. Note stan fits result slightly difference numbers, random seeds . Mostly, model 1, 3 4 selected using WAIC, similarly good models. indicative Model 0 less plausible model. confident unlikely influence dependent variables individual differences. following code creates data frame plot model comparison together. graphic comparison show qualitatively Model 1, 3, 4 indistinguishable Model 0 2 also indistinguishable. significant result two groups first group preferred. select Model 1 first group ground minimal, parsimonious model. addition relying abstract information criterion, WAIC, examine fits model preditions. big circles show average values (across participants). numbers top circles show number responses, clear many data points condition models able optimise . small circles show responses participant. Model 0 Model 1 predictions represented red grey, respectively. Three important findings can concluded. First, data consistent argument participants always decided wait made post-car responses, one looks data 2.5-s TTA, 99.7% safe responses, made car passed, believe argument, one must overlook data 3-s TTA condition. Participants made good number responses car passes way know whether car arrive 2.5-s 3-s trial, let alone TTA made slightly randomly deviated set time. , trials TTA conditions appeared randomly. Participants must carefully evaluate imperative stimulus; otherwise, data show four TTA conditions, , , responses made car passes. Response proportion made car passes Second, L-R factor influence behavioural responses. conclusion supported analysis Model1, 3, 4 show equally good fit data Model 2 similar baseline model (ie Model 0). Third, TTA factor strong influence participants’ response proportions choosing respond car passes well ability make safe crosses car passes, although makes difference proportion safe crosses car passes. Four, TTA factor also strong influence participants’ response latency. clear responses made car passes, ambiguous responses made car passes, possibly impact. Bayesian model conventionally provides probabilistic information data code transparent anyone use, wish leave read decide matter. TTA factor seems impact latency responses resulting collisions. impact, however, strong dependent variables, WAIC difference big others. model fit data suggest can believe TTA affect collision RT. observation practically important may inform policy pay attention TTA collision prevention. recommend considered significant. , reader decide examine analyses report re-analyse data. concludes vignette. next step, employ four sequence-sampling models attempt explain dependent variables one framework compare difference make inferences plausible psychological process.","code":"rt <- d[R == \"safe\", .(count = .N,                        value = median(RT)), .(TTAint, TTANme, side, G, s)] dbe <- rt[G == \"before\"]  daf <- rt[G == \"after\"]   rt <- d[R == \"hit\", .(count = .N,                       value = median(RT)), .(TTAint, TTANme, side, G, s)] dbe_hit <- rt[G == \"before\"]  daf_hit <- rt[G == \"after\"]   m0 <- map2stan(                             alist(     value ~ dnorm(mu, sigma),     mu <- a[s],     a[s] ~ dnorm(amu, sigma_a),                amu ~ dnorm(0, 10),       c(sigma, sigma_a) ~ dcauchy(0, 2)   ),   data = data.frame(dbe), iter = 5000, warmup = 2500, chains = 4, cores = 4)  m1 <- map2stan(   alist(     value ~ dnorm(mu, sigma),     mu <- a[s] + b1 * TTAint,     a[s] ~ dnorm(amu, sigma_a),                 amu ~ dnorm(0, 10),     b1 ~ dnorm(0, 10),     c(sigma, sigma_a) ~ dcauchy(0, 2)   ),   data = data.frame(dbe), iter = 5000, warmup = 2500, chains = 4, cores = 4)  m2 <- map2stan(   alist(     value ~ dnorm(mu, sigma),     mu <- a[s] + b2 * side,     a[s] ~ dnorm(amu, sigma_a),                 amu ~ dnorm(0, 10),     b2 ~ dnorm(0, 10),     c(sigma, sigma_a) ~ dcauchy(0, 2)   ),   data = data.frame(dbe), iter = 5000, warmup = 2500, chains = 4, cores = 4)  m3 <- map2stan(   alist (     value ~ dnorm(mu, sigma),     mu <- a[s] + b1 * TTAint +  b2 * side,     a[s] ~ dnorm(amu, sigma_a),                 amu ~ dnorm(0, 10),     b1 ~ dnorm(0, 10),     b2 ~ dnorm(0, 10),     c(sigma, sigma_a) ~ dcauchy(0, 2)   ),   data = data.frame(dbe), iter = 5000, warmup = 2500, chains = 4, cores = 4)  m4 <- map2stan(   alist (     value ~ dnorm(mu, sigma),     mu <- a[s] + b1 * TTAint +  b2 * side + b3 * TTAint * side,     a[s] ~ dnorm(amu, sigma_a),     amu ~ dnorm(0, 10),     b1 ~ dnorm(0, 10),     b2 ~ dnorm(0, 10),     b3 ~ dnorm(0, 10),     c(sigma, sigma_a) ~ dcauchy(0, 2)   ),   data = data.frame(dbe), iter = 5000, warmup = 2500, chains = 4, cores = 4)  # save(m0, m1, m2, m3, m4, file = \"tests/2_preprocess/data/logistic_rt_before.RData\") # save(m0, m1, m2, m3, m4, file = \"tests/2_preprocess/data/logistic_rt_after.RData\") # save(m0, m1, m2, m3, m4, file = \"tests/2_preprocess/data/logistic_rt_before_hit.RData\") load(\"tests/2_preprocess/data/logistic_precar.RData\") ms0 <- compare(m0, m1, m2, m3, m4) load(\"tests/2_preprocess/data/logistic_safe_before.RData\") ms1 <- compare(m0, m1, m2, m3, m4) load(\"tests/2_preprocess/data/logistic_safe_after.RData\") ms2 <- compare(m0, m1, m2, m3, m4) load(\"tests/2_preprocess/data/logistic_rt_before.RData\") ms3 <- compare(m0, m1, m2, m3, m4) load(\"tests/2_preprocess/data/logistic_rt_after.RData\") ms4 <- compare(m0, m1, m2, m3, m4) load(\"tests/2_preprocess/data/logistic_rt_before_hit.RData\") ms5 <- compare(m0, m1, m2, m3, m4) # plot(ms5) c(ms0, ms1, ms2, ms3, ms4, m5)  # Pre-car proportion | safe #    WAIC pWAIC dWAIC weight    SE   dSE # m3 7148  38.6   0.0   0.65 79.58    NA # m4 7150  39.8   1.4   0.33 79.76  2.21 # m1 7155  37.8   6.4   0.03 79.49  5.65 # m2 7926  37.9 777.6   0.00 65.78 54.66 # m0 7932  37.0 783.8   0.00 65.70 54.98 #  # Safe proportion | made before  #    WAIC pWAIC dWAIC weight    SE   dSE # m3 1153  39.2   0.0   0.69 68.60    NA # m4 1155  40.6   2.1   0.24 69.20  1.79 # m1 1158  38.1   4.7   0.07 68.80  5.33 # m2 1894  38.6 740.8   0.00 83.72 57.64 # m0 1906  37.3 753.0   0.00 83.81 58.07 #  # Safe proportion | made after  #    WAIC pWAIC dWAIC weight    SE  dSE # m0 88.7   7.4   0.0   0.45 28.42   NA # m2 88.9   8.0   0.2   0.40 28.48 0.39 # m1 92.3   8.8   3.6   0.07 29.90 2.03 # m3 92.4   9.4   3.8   0.07 30.02 2.57 # m4 97.6  11.6   8.9   0.01 32.85 7.12 #  # RT | made before  #      WAIC pWAIC dWAIC weight    SE   dSE # m1 -478.4  37.8   0.0   0.69 52.63    NA # m3 -475.9  38.5   2.5   0.20 52.39  1.41 # m4 -474.7  40.6   3.8   0.11 54.61  5.30 # m0 -417.8  36.0  60.6   0.00 51.95 17.61 # m2 -415.8  36.9  62.6   0.00 51.55 17.48 #  # RT | made after #      WAIC pWAIC  dWAIC weight    SE   dSE # m1 -735.0  34.5    0.0   0.49 27.73    NA # m3 -734.3  35.3    0.7   0.34 27.76  2.36 # m4 -733.0  36.4    2.1   0.17 27.94  3.71 # m0  567.4   3.2 1302.5   0.00 14.51 30.68 # m2  569.1   4.0 1304.2   0.00 14.64 30.77  # RT | made before, collision # WAIC pWAIC dWAIC weight    SE   dSE # m1 120.3  22.6   0.0   0.60 53.74    NA # m3 121.5  23.9   1.1   0.34 53.26  1.68 # m4 125.2  25.9   4.8   0.05 53.54  3.16 # m0 254.8  14.6 134.4   0.00 41.96 26.99 # m2 256.1  15.0 135.8   0.00 41.91 26.69  # Examine stanfit object in each model object to see Rhat all around 1's. m0@stanfit dms0 <- data.table(ms0@output) dms1 <- data.table(ms1@output) dms2 <- data.table(ms2@output) dms3 <- data.table(ms3@output) dms4 <- data.table(ms4@output) dms5 <- data.table(ms5@output) dms0$M <- \"Pre-car proportion | safe\" dms1$M <- \"Safe proportion | made before\" dms2$M <- \"Safe proportion | made after\" dms3$M <- \"RT | made before\" dms4$M <- \"RT | made after\" dms5$M <- \"RT | made before, collision\"  dms0$m <- factor(rownames(ms0@output),                   levels = rownames(ms0@output)[order(dms0$WAIC)]) dms1$m <- factor(rownames(ms1@output),                   levels = rownames(ms1@output)[order(dms1$WAIC)]) dms2$m <- factor(rownames(ms2@output),                   levels = rownames(ms2@output)[order(dms2$WAIC)]) dms3$m <- factor(rownames(ms3@output),                   levels = rownames(ms3@output)[order(dms3$WAIC)]) dms4$m <- factor(rownames(ms4@output),                   levels = rownames(ms4@output)[order(dms4$WAIC)]) dms5$m <- factor(rownames(ms5@output),                   levels = rownames(ms5@output)[order(dms5$WAIC)]) dms <- rbind(dms0, dms1, dms2, dms3, dms4, dms5) Models <- c(\"Pre-car proportion | safe\", \"Safe proportion | made before\",             \"Safe proportion | made after\", \"RT | made before\", \"RT | made after\",             \"RT | made before, collision\") dms$M <- factor(dms$M, levels = Models) p <- ggplot(data = dms) +   geom_point(aes(x = m, y = WAIC)) +   geom_errorbar(aes(x = m, y = WAIC, ymin = WAIC + SE, ymax = WAIC - SE),                 width = .5)+   facet_wrap(M~., scales = \"free\") +   xlab(\"Model\") + ylab(\"Deviance\") +   theme_bw(base_size = 18)  png(\"tests/inst/figs/10_models_v1.png\", 800, 600) p dev.off()"},{"path":[]},{"path":[]},{"path":"/articles/web_only/2_input_only_v0.html","id":"a-detour-to-the-lba-model","dir":"Articles > Web_only","previous_headings":"Drift-rate Model","what":"A Detour to the LBA model","title":"2. Constant Drift-rates","text":"assuming inputs previous neural layer influence drift rate, \\(dx_i\\), can write, \\(dx_i = \\rho_i \\frac{dt}{\\tau}\\). results similar case assumed LBA model (Brown Heathcote 2008). LBA model assumes within-trial variability (time) uses -trial variability match performance data across many trials contributed participant. , samples trial--trial drift rates, \\(\\rho_i\\) Gaussian distribution, \\(N(\\mu_j, \\sigma)\\) trial--trial starting activations uniform distribution, \\(U(0, )\\). (\\(j\\) index accumulator \\(\\), trial). \\[ \\rho_i \\sim N(\\mu_j, \\sigma) \\\\ x_{0_i} \\sim U(0, ) \\]","code":""},{"path":"/articles/web_only/2_input_only_v0.html","id":"drift-rate-model-1","dir":"Articles > Web_only","previous_headings":"Drift-rate Model","what":"Drift-rate Model","title":"2. Constant Drift-rates","text":"Back LCA equation. Without stochastic component, can calculate exactly many steps, given distance (.e., threshold, denoted \\(Z\\)), accumulator take. \\(Z\\) fixed 2.8 following example. use awkward name, dtovertau, remind non-trivial nuance. time step often denoted \\(dt\\), must scaled natural time unit, one wish investigate association natural times (often taken PC, software, relevant hardware also involve different measurement imprecision) bypass likelihood calculations tricks (e.g., using proportionality way). scaling parameter denoted \\(\\tau\\). simulation, often convenient use discrete unit step, instance, (size_t = 0; < nstep; ++). example, took first accumulator, receives large input, 57 steps pass threshold 2.8. 56th step reached 2.7999 activation, slightly lower 2.8. Without noise term, deterministic accumulator one chosen. \\[dx_i = \\rho_i  \\frac{dt}{\\tau} +  \\xi_i \\sqrt{ \\frac{dt}{\\tau} } \\] following lists basic structure R code implementing equation. coded style, making easier converted C++ readable. figures upper row show C++ implementation results accumulation process R implementation. figures lower row added noise thus, accumulator two sometimes chosen. lesson without stochastic component, difference drift rates decides accumulator chosen. Activation trajectories","code":"dtovertau <- 0.1 dt <- 10  # 10 ms tau <- dt/dtovertau; tau # 100  sdv <- sqrt(dtovertau); sdv  ## Parameters  x0 <- c(0, 0)         # A; starting evidence values  Z  <- 2.8             # b; common threshold across the xi t0 <- .2              # NDT I  <- c(0.5, 0.385)   # inputs from a previous layer = drift rate  maxiter   <- 5001  ## Prepare the process  nacc      <- length(I) output    <- numeric(2) undone    <- TRUE counter   <- 0 random    <- F nonlinear <- F  ## T or F does not matter for this set of parameters  ## initialise accumulator array to record activation value dx <- numeric(nacc);    activation <- x0  at_mat <- matrix(nrow = maxiter, ncol = nacc) dx_mat  <- matrix(nrow = maxiter, ncol = nacc)  at_mat[counter+1,] <- activation dx_mat[counter+1, ] <- 0  repeat {   counter <- counter + 1   for (i in seq_len(nacc)) {          dx[i] <- dtovertau * I[i]     activation[i] <- activation[i] + dx[i]          if(random) activation[i] <- activation[i] + rnorm(1, 0, sdv)          if (activation[i] >= Z) {        output[1] <- counter * dt - 0.5*dt + t0[i];       output[2] <- i;        undone    <- FALSE;        message(\"Activation value: \", activation[i])     }     if (nonlinear && activation[i] < 0) activation[i] <- 0;   }      ac_mat[counter+1, ] <- activation   dx_mat[counter+1,]  <- dx    if (counter > maxiter || !undone) break; }  # counter's first index store step 0 2.8/ (dtovertau*I) counter ## 57"},{"path":"/articles/web_only/2_input_only_v0.html","id":"free-response-paradigm","dir":"Articles > Web_only","previous_headings":"Drift-rate Model","what":"Free Response Paradigm","title":"2. Constant Drift-rates","text":"Free response paradigm refers design task permits decision makers make choice without worrying time limit. , free respond time (interpretation). term used (Bogacz et al. 2006), follow terminology used , hoping standard ubiquitous term researchers can understand. implementing model, design corresponds idea absorbing boundary (Feller 2008), another specialised term, meaning accumulator terminates activity reaches threshold thus, metaphorically absorbed boundary.","code":""},{"path":"/articles/web_only/2_input_only_v0.html","id":"choice-probability","dir":"Articles > Web_only","previous_headings":"Drift-rate Model > Free Response Paradigm","what":"Choice Probability","title":"2. Constant Drift-rates","text":"noise, always first accumulator chosen. examined drift-rate difference two accumulators. Fixing drift rate accumulator 1 0.5, differences 0.10 0.13 0.17 0.20 0.23 0.27 0.30 0.33 0.37 0.40. assume accumulator 1 matches target. probability chosen first option larger difference, probable first accumulator chosen. x facet shows drift rate difference subtracting value second accumulator first accumulator. model driven input drift rate, unsurprising see distributions Gaussian shape mean 0 standard deviation sqrt(dtovertau).","code":"# rm(list = ls()) # p.vector <- c(Z = 2.8, t0 = .2, dtovertau = 0.1, dt = 10) # Ilist <- vector(\"list\", length = 10) # I2 <- seq(.4, .1, length.out = 10) # for(i in seq_len(10)) { #      # Imat[i, ] <- c(.5, I2[i])  #        Ilist[[i]] <- c(.5, I2[i])  #      } # fun <- function(I, p.vector, n = 500, nboot = 1000) { #     out <- numeric(nboot) #     for(i in seq_len(nboot)) { #          res <- rInput(n = n, par = p.vector, I = I, x0 = c(0, 0), 5001, F,  #                                             random = T, debug = F) #              tmp <- table(res@choice_RT[2,]) / n #              out[i] <- tmp[1] #          } #        return(out) # } #  # ncore <- length(Ilist) # out <- parallel::mclapply(Ilist, fun, p.vector, n = 10000, nboot = 1000, #                           mc.cores = ncore)  # save(out, file = \"tests/6_LCA/data/dprob.rda\")  tmp <- matrix(nrow = 10, ncol = 1000) I2 <- seq(.4, .1, length.out = 10) x0 <- NULL for(i in 1:10) {   tmp[i, ] <- out[[i]]   tmp2 <- data.table(x = out[[i]], I = factor(round(I2[i], 2)))   x0 <- rbind(tmp2, x0) }  dprob <- x0  dvline <- data.table(x = rowMeans(tmp), y = 60,                       I =  factor(round(I2, 2)),                       lab = round(rowMeans(tmp), 2)) binsize <- 5 * sd(out[[1]]) / sqrt(length(out[[1]]))  p2 <- ggplot() +   geom_histogram(data = dprob, aes( x = x), binwidth = binsize) +   geom_vline(data = dvline, aes(xintercept = x)) +   geom_text(data = dvline, aes(x = x, y = y, label = lab),               size = 9) +   facet_wrap(I~., scales = \"free_x\") +   theme_bw(base_size = 18) p2 fig <- ggplotly(p2)"},{"path":"/articles/web_only/2_input_only_v0.html","id":"response-time-distributions","dir":"Articles > Web_only","previous_headings":"Drift-rate Model > Free Response Paradigm","what":"Response Time Distributions","title":"2. Constant Drift-rates","text":"check numerical details figure, use wonderful plotly package. choice RT distribution using inputs .5 .3 summary, variant model pair reasonable choice RT distributions.","code":"## RT Distribution ----------- plot_hist <- function(x, binwidth = .1, cols = c(\"RT\", \"R\"), print.fig = TRUE) {   upper <- max(x$RT) + 3 * binwidth   bk <- seq(0, upper, by = binwidth)   dtmp <- x[, ..cols]   dA <- dtmp[R == 1, ..cols]   dB <- dtmp[R == 2, ..cols]   rtA <- dA$RT   rtB <- dB$RT   y0 <- cut(rtA, breaks = bk)   y1 <- cut(rtB, breaks = bk)   tmp0 <- cbind(lower = as.numeric( sub(\"\\\\((.+),.*\", \"\\\\1\", y0) ),                 upper = as.numeric( sub(\"[^,]*,([^]]*)\\\\]\", \"\\\\1\", y0) ))   tmp1 <- cbind(lower = as.numeric( sub(\"\\\\((.+),.*\", \"\\\\1\", y1) ),                 upper = as.numeric( sub(\"[^,]*,([^]]*)\\\\]\", \"\\\\1\", y1) ))   dA$mid <- .5*rowSums(tmp0)   dB$mid <- .5*rowSums(tmp1)      d1 <- dA[, .(y = .N / nrow(dtmp)), .(mid)]   d2 <- dB[, .(y = .N / nrow(dtmp)), .(mid)]   names(d1) <- c(\"x\", \"y\")   names(d2) <- c(\"x\", \"y\")   d1$R <- \"1\"   d2$R <- \"2\"   tmp2 <- rbind(d1, d2)      p2 <- ggplot(data = tmp2) +     geom_line( aes(x = x, y = y, colour = R), size = 1) +     geom_point(aes(x = x, y = y, colour = R), size = 2) +     xlab(\"RT (s)\") +  ylab(\"Proportion of trials\") +     theme_minimal(base_size = 20)    if (print.fig) print(p2)   return(tmp2) }  n <- 3000 p.vector <- c(Z = 2.8, t0 = .2, dtovertau = 0.1, dt = 10) res5 <- rInput(n = n, par = p.vector, I = c(.5, .25), x0 = c(0, 0), 5001, F,               random = T, debug = F) dRT <- data.table(RT = res5@choice_RT[1,]/1000, R =                     factor(res5@choice_RT[2,]))   tmp <- plot_hist(dRT, binwidth = .09)  x0 <- NULL n <- 3000 for(i in 1:1000) {   res5 <- rInput(n = n, par = p.vector, I = c(.5, .25), x0 = c(0, 0), 5001, F,                 random = T, debug = F)   tmp <- data.table(RT = res5@choice_RT[1,]/1000, R =                        factor(res5@choice_RT[2,]))   tmp$boot <- factor(i)   x0 <- rbind(x0, tmp) }  cols <- c(\"RT\", \"R\") x0[boot == 1, ..cols] x1 <- NULL for(i in seq_len(1e3)) {   tmp <- plot_hist(x0[boot == i, ..cols], binwidth = .09, print.fig = FALSE)   tmp$boot <- factor(i)   x1 <- rbind(x1, tmp) }  save(x0, x1, file = \"tests/6_LCA/data/distribution.rda\") x1$Rboot <- paste0(x1$R, x1$boot) dtmp <- x1[, .(N = .N,        y = mean(y)), .(x, R)] dtmp2 <- dtmp[, c(\"x\", \"y\", \"R\")] dtmp2$Rboot <- paste0(dtmp$R, \"Avg\")  # table(x1$x, x1$R) x1[boot == 1]  p3 <- ggplot(data = x1) +   geom_line( aes(x = x, y = y, group = Rboot, colour = R), size = 1, alpha = .3) +   geom_line(data = dtmp2, aes(x = x, y = y, group = Rboot), size = 1) +   # geom_point(aes(x = x, y = y, colour = R), size = 2) +   xlab(\"RT (s)\") +  ylab(\"Proportion of trials\") +   theme_minimal(base_size = 20)  p3  png(file = \"vignettes/web_only/images/LBA-3.png\", width = 800, height = 600) print(p3) dev.off() fig <- ggplotly(p3) fig"},{"path":"/articles/web_only/2_input_only_v0.html","id":"time-limited-paradigm","dir":"Articles > Web_only","previous_headings":"Drift-rate Model","what":"Time-limited Paradigm","title":"2. Constant Drift-rates","text":"Time-limited paradigm refers task design contrast free-response paradigm. free-response paradigm, response (time) window usually set far longer people’s slowest response time time limit . time-limited paradigm, observer given limited amount time make choice. time likely sufficient make high confidence choice many people. modelling process, can implemented large threshold monitor accumulator garnered evidence others time step. probability choosing first accumulator eventually reach 1, given, e.g., 10000 steps. choice RT distribution using inputs .5 .3","code":"## What is the choice probability ? rho <- c(.5, .4) x0 <- c(0, 0) p.vector <- c(Z = 1000, t0 = .2, dtovertau = 0.1, dt = 10) maxiter <- 10000 ncore <- 6  ntrial <- rep(5000, ncore) nlist <- vector(\"list\", length = ncore) for(i in seq_len(ncore)) {  nlist[[i]] <- ntrial[i] } # nlist  fun <- function(ntrial, p.vector, rho, x0, maxiter) {   # cat(ntrial, \"\\n\")   res0 <- rInput(n = ntrial, par = p.vector, I = rho, x0 = x0,                 maxiter = maxiter, nonlinear = F, random = T, debug = F)    out <- matrix(nrow = maxiter+1, ncol = ntrial)   for(i in seq_len(ntrial)) {     x1 <- res0@activation[1, 1:(maxiter+1) , i]     x2 <- res0@activation[2, 1:(maxiter+1) , i]     out[,i] <- (x1 > x2)   }   return(out) }"},{"path":"/articles/web_only/2_input_only_v0.html","id":"recovery-studies","dir":"Articles > Web_only","previous_headings":"Drift-rate Model","what":"Recovery Studies","title":"2. Constant Drift-rates","text":"used generic method, probability density approximation derive model likelihoods used maximum likelihood estimation, instead Bayesian inference, conduct recovery studies. latter also uses likelihood functions, usually uses different method propose candidate parameters estimate estimation intervals. former enough provide answers regarding whether: approximated likelihood function precise enough recover parameters case LCA reduced LCA models. model recoverable range parameters interested . implement process model based previous simplified LCA model (cousin LBA model), called rInput_internal.","code":"pkg <- c(\"ggplot2\", 'data.table', 'pedxing', 'rbenchmark', 'nloptr', 'plotly') sapply(pkg, require, character.only = T) setwd(\"/media/yslin/Tui/projects/pedxing/\") rm(list = ls()) pal <- Manu::get_pal(\"Kaka\") get_sll <- function(x) { sum( log(x[[1]] ) ) + sum( log(x[[2]] ) ) }"},{"path":"/articles/web_only/2_input_only_v0.html","id":"drift-rate-model-2","dir":"Articles > Web_only","previous_headings":"Drift-rate Model > Recovery Studies","what":"Drift-rate model","title":"2. Constant Drift-rates","text":"data set used evaluate recovery simulated data 100 trials. choice-RT data number preliminary model fitting gauge well subplex routine particular one implemented sbplx performs. linking newly written rInput_internal function spdf routine pda.cpp ggdmc, simulated dLBA function. ideal negative log-likelihood 63. minimisation routine can find parameters, optimisation around data, return negative log-likelihood close value. LCA model non-negative constraint, apply parameters, restricting parameter space. suggested also early LCA paper (Usher McClelland 2001). following shows one example run. proceeding fit model data, conducted parameter survey 2. calculated model likelihoods constructing grid pair drift rates two accumulators. range goes -2 2, using step 0.051. purpose surveying values 0 preliminary tests fitting model simulated data set, found fair percentage (25 %) 100-run bootstrapping test estimated drift rates near 0, given set lower bound 0 parameters knew true values two drift rates 0.5 0.2. result summarised following figure. orange circles shows two discontinuous regions return higher likelihoods purple circles close true values. Two features discontinuous regions stands : one drift rates negative orange circles one drift rates close 0 green circles. Note green circles represent parameters resulting lower likelihoods purple circles, parameters close one resulting maximum likelihood. likely parameter search routine might still end reporting best estimate sampling one green circles two discontinuous regions, jump one low likelihood points near edge concave surface relatively high likelihood point discontinuous regions. PDA method increases likelihood search scenario attempt simplex real maximum likelihood region returns low likelihood noisy likelihood estimation thereby guides search process away maximum likelihood region (see similar problem Bayesian inference (Holmes 2015). summary, resolve problem setting lower bound 0 parameter search routine employ five parallel searches. line theoretical assumption LCA model, race-style accumulation models, assume positive drift rates. employed five parallel searches set zero lower bound parameter search routine resolve problem. Parameter space","code":"p.vector <- c(Z = 2.8, t0 = .2, dtovertau = 0.1, dt = 10/1000) rho <- c(.5, .35) n <- 100 tmp0 <- rInput_internal(n = n, par = p.vector, I = rho, x0 = c(0, 0), 5001,                        nonlinear = F, random = T)  dat <- data.table(RT = tmp0[1,], R = tmp0[2,])  dat_tmp <- data.table(RT = tmp0[1,], R = tmp0[2,]+1)  png(file = \"vignettes/web_only/images/LBA-5.png\", width = 800, height = 600) plot_hist(dat_tmp, binwidth = .3) dev.off() get_sll <- function(x) { sum( log(x[[1]] ) ) + sum( log(x[[2]] ) ) }  tmp1 <- dLBA(dat$RT, dat$R, nsim = 10000, par = p.vector, I = rho,               x0 = c(0, 0), 5001, F, T, F) -get_sll(tmp1) ##  63.25775 ## dLBA --------------------------- fun <- function (rho, data, debug) {   require(data.table)   # cat(\" \", rho, \"\\n\")   test_rho <<- rbind(test_rho, rho[1:2])   setorder(data, R, RT)   RT <- data$RT   R <- data$R    p.vector <- c(Z = rho[3], t0 = .2, dtovertau = 0.1, dt = 10/1000)    tmp <- dLBA(RT, R, nsim = 10000, par = p.vector, I = rho[1:2], x0 = c(0, 0),                5001, F, T, debug)   out <- -get_sll(tmp)   test_nll <<- c(test_nll, out)   return(out)  }  nmaxit <- 1000 test_rho <- NULL test_nll <- NULL fit <- sbplx(x0 = c(1, 1, 1), fn = fun,               lower = c(0, 0, 0), data = dat, debug = FALSE,                   control=list(maxeval = nmaxit))  dresult <- data.table( true = c(rho, p.vector[1], -get_sll(tmp1)),                         estm = c(fit$par, test_nll[fit$iter]) ) dresult #        true       estm # 1:  0.50000  0.4831009 # 2:  0.35000  0.1935556 # 3:  2.80000  2.5724348 # 4: 63.99076 63.6569213"},{"path":"/articles/web_only/2_input_only_v0.html","id":"start-the-searches","dir":"Articles > Web_only","previous_headings":"Drift-rate Model > Recovery Studies > Drift-rate model","what":"Start the searches","title":"2. Constant Drift-rates","text":"Parameter space estimates perfect 2% chance returns parameters resulting high likelihoods, high maximum. Nevertheless, bootstrapped distribution suggest good chance estimate drift rate 1 larger drift rate 2, threshold estimate close true value. sum , now basic LCA formulation. can add leakage term see affects model prediction see can make something can use task manifest signal suppression good hypothesis.","code":"run_fun <- function(i, n) {   # i <- 1   # n <- 100   if(i %% 10 == 0) cat(\"step \", i, \"\\n\")         fun <- function (par, data, debug) {     require(data.table)          test_par <<- rbind(test_par, par)          setorder(data, R, RT)     RT <- data$RT     R <- data$R      p.vector <- c(Z = par[3], t0 = .2, dtovertau = 0.1, dt = 10/1000)          tmp <- dLBA(RT, R, nsim = 10000, par = p.vector, I = par[1:2], x0 = c(0, 0),                 5001, F, T, debug)     out <- -get_sll(tmp)     test_nll <<- c(test_nll, out)     return(out)        }      p.vector <- c(Z = 2.8, t0 = .2, dtovertau = 0.1, dt = 10/1000)   rho <- c(.5, .35)   tmp0 <- rInput_internal(n = n, par = p.vector, I = rho, x0 = c(0, 0), 5001,                         nonlinear = F, random = T)   dat <- data.table(RT = tmp0[1,], R = tmp0[2,])      npar <- 3      x0 <- runif(npar)   x1 <- runif(npar)   x2 <- runif(npar)   x3 <- runif(npar)   x4 <- runif(npar)   lower <- rep(0, npar)      test_par <- NULL   test_nll <- NULL   fit0 <- sbplx(x0 = x0, fn = fun,                 lower = lower, data = dat, debug = FALSE)   res0 <- list(fit = fit0, parameter = test_par, nll = test_nll, data = dat)    test_par <- NULL   test_nll <- NULL   fit1 <- sbplx(x0 = x1, fn = fun,                  lower = lower, data = dat, debug = FALSE)   res1 <- list(fit = fit1, parameter = test_par, nll = test_nll, data = dat)    test_par <- NULL   test_nll <- NULL   fit2 <- sbplx(x0 = x2, fn = fun,                  lower = lower, data = dat, debug = FALSE)   res2 <- list(fit = fit2, parameter = test_par, nll = test_nll, data = dat)    test_par <- NULL   test_nll <- NULL   fit3 <- sbplx(x0 = x3, fn = fun,                  lower = lower, data = dat, debug = FALSE)   res3 <- list(fit = fit3, parameter = test_par, nll = test_nll, data = dat)    test_par <- NULL   test_nll <- NULL   fit4 <- sbplx(x0 = x4, fn = fun,                  lower = lower, data = dat, debug = FALSE)   res4 <- list(fit = fit4, parameter = test_par, nll = test_nll, data = dat)      fits <- list(fit0, fit1, fit2, fit3, fit4)    idxmin <- which.min(c(fit0$value, fit1$value, fit2$value, fit3$value, fit4$value))   return(fits[[idxmin]]) }  ncore <- 10 t0 <- Sys.time() res <-  parallel::mclapply(1:100, run_fun, n = 100, mc.cores = ncore) t1 <- Sys.time() save(res, file = \"tests/6_LCA/data/test_dLBA_v9.rda\")  true <- c(.5, .35, 2.8) nll <- numeric(100) parameter <- matrix(nrow = 100, ncol = 3) difference <- matrix(nrow = 100, ncol = 3) # i <- 1 for(i in 1:100) {   parameter[i,] <- res[[i]]$par   difference[i,] <- res[[i]]$par - true   nll[i] <- res[[i]]$value      p.vector <- c(Z = 2.8, t0 = .2, dtovertau = 0.1, dt = 10/1000)   p.vector[1] <- res[[i]]$par[3] }  # save(res, parameter, difference, nll, file = \"tests/6_LCA/data/test_dLBA_v9.rda\")  load(\"tests/6_LCA/data/test_dLBA_v9.rda\")  dpar <- data.table(parameter, nll = nll) names(dpar) <- c(\"rho1\", \"rho2\", \"Z\", \"nll_subplex\") round(dpar, 2) dtmp1 <- dpar[nll_subplex > 75.2] dtmp2 <- dpar[nll_subplex <= 75.2]  # The relative standing is kept. No estimates report that rho1 is greater # than rho2, in line with the true values sum( dpar$rho1 > dpar$rho2 ) # 0  # Only two estimates amongst 100 attempts were from the edges of the  # surface  round(dtmp2, 2) #    rho1 rho2    Z nll_subplex # 1: 0.05 0.02 1.16       95.54 # 2: 0.14 0.04 1.16       75.43  nrow(dtmp1)/nrow(dpar) nrow(dtmp2)/nrow(dpar) sum( dtmp2$rho1 > dtmp2$rho2 ) # 98  psych::describe(dtmp1$nll_subplex) #    mean    sd median trimmed   mad   min   max range skew kurtosis    se #  50.93   9.55  51.33   51.06   9.7 30.29 75.12 44.83 -0.02    -0.54 0.96  bw1 <- 3*plotrix::std.error(dtmp2$rho1) bw2 <- 3*plotrix::std.error(dtmp2$rho2) bw3 <- 3*plotrix::std.error(dtmp2$Z) p <- ggplot() +   geom_histogram(data = d1[gp == 1], aes(x = x), binwidth = bw1) +   geom_histogram(data = d1[gp == 2], aes(x = x), binwidth = bw2) +   geom_histogram(data = d1[gp == 3], aes(x = x), binwidth = bw3) +   geom_vline(data = dvline, aes(xintercept = x, color = gp)) +   facet_wrap(.~gp, scales = \"free\")  # png(file = \"vignettes/web_only/images/dLBA-1.png\", width = 800, height = 600) # p # dev.off()"},{"path":"/articles/web_only/2_input_only_v0.html","id":"programming-note","dir":"Articles > Web_only","previous_headings":"","what":"Programming Note","title":"2. Constant Drift-rates","text":"used five packages support code vignette. pedxing customised package associated manuscript work. going hosted Github.","code":"pkg <- c(\"ggplot2\", 'data.table', 'pedxing', 'rbenchmark', 'plotly') sapply(pkg, require, character.only = T) setwd(\"/media/yslin/Tui/projects/pedxing/\") rm(list = ls())"},{"path":[]},{"path":"/articles/web_only/3_leak_v0.html","id":"inhibition-model","dir":"Articles > Web_only","previous_headings":"","what":"Inhibition model","title":"2. Comparing the Input-only model to the Leakage and Inhibition Models","text":"inhibition model equation: \\[ dx_i = (I_i - \\beta \\sum_{' \\neq } f_{'} - \\lambda x_i ) \\frac{dt}{\\tau} +  \\xi_i \\sqrt{ \\frac{dt}{\\tau} } \\] omit additional assumption self-excitatory input, \\(\\alpha f_i\\), assumes neuron feedbacks excitatory signal. figure shows pair functions, one can described inverse negative power function multiplying \\(-1\\). Comparing three models without stochastic influence. close look drift rate shows , leakage inhibition unit shows magnitude leakage inhibition model minimal, comparing inhibition unit. , unit larger input exerts stronger inhibition unit. time, inhibition signal grows stronger, resulting drift rate even going negative. simulation apply non-linear cap negative activation. Comparing three models without stochastic influence. Test Comparing three models without stochastic influence.","code":"x <- seq(0, 1, .1) y1 <- 1/x^(-3) y2 <- -1/x^(-3) yl <- range(y1,y2) plot(x, y1, ylim = yl, type = \"l\") lines(x, y2)"},{"path":"/articles/web_only/3_leak_v0.html","id":"equation-2","dir":"Articles > Web_only","previous_headings":"Inhibition model","what":"Equation (2)","title":"2. Comparing the Input-only model to the Leakage and Inhibition Models","text":"second equation introduces inhibitory influences exerted accumulators ego accumulator. authors first stated inhibitory input others ego accumulator weighted sum outputs others accumulators, assumed simplified assumption weight units , denoted \\(\\beta\\). elegant way write equation \\(\\beta \\sum_{' \\neq } f_{'}\\). exactly implemented. equation (2) finally becomes:","code":""},{"path":"/articles/web_only/3_leak_v0.html","id":"note-on-symbols-in-equations","dir":"Articles > Web_only","previous_headings":"","what":"Note on Symbols in Equations","title":"2. Comparing the Input-only model to the Leakage and Inhibition Models","text":"\\[ dx_i = ( \\rho_i + \\alpha f_i  - \\beta \\sum_{' \\neq } f_{'} - \\lambda x_i ) \\frac{dt}{\\tau} +  \\xi_i \\sqrt{ \\frac{dt}{\\tau} } \\] ⚠\\(\\rho_i\\) equation (2) equal \\(I_i\\)⚠. \\(\\rho_i\\) represents part \\(I_i\\). , \\(^{ext}_i\\), excitatory input previous layer. Term 1 (\\(\\rho\\)), term 2 (\\(\\alpha f_i\\)), term 3 (\\(\\beta \\sum_{' \\neq } f_{'}\\)) lumped together \\(I_i\\). \\(\\lambda x_i\\) represents leakage activation.","code":""},{"path":[]},{"path":[]},{"path":"/articles/web_only/4_LCA_v0.html","id":"a-detour-to-the-lba-model","dir":"Articles > Web_only","previous_headings":"Drift-rate Model","what":"A Detour to the LBA model","title":"2. Leaky Competing Accumulation Model","text":"assuming inputs previous neural layer influence drift rate, \\(dx_i\\), can write, \\(dx_i = \\rho_i \\frac{dt}{\\tau}\\). results similar case assumed LBA model (Brown Heathcote 2008). LBA model assumes within-trial variability (time) uses -trial variability. , samples trial--trial drift rates, \\(\\rho_i\\) Gaussian distribution, \\(N(\\mu_j, \\sigma)\\) trial--trial starting activations uniform distribution, \\(U(0, )\\). (\\(j\\) index accumulator \\(\\), trial). \\[ \\rho_i \\sim N(\\mu_j, \\sigma) \\\\ x_{0_i} \\sim U(0, ) \\]","code":""},{"path":"/articles/web_only/4_LCA_v0.html","id":"drift-rate-model-1","dir":"Articles > Web_only","previous_headings":"Drift-rate Model","what":"Drift-rate Model","title":"2. Leaky Competing Accumulation Model","text":"Back LCA equation. Without stochastic component, can calculate exactly many steps, given distance (.e., threshold, denoted \\(Z\\)), accumulator take. \\(Z\\) fixed 2.8 following example. use bit awkward variable dtovertau, remind non-trivial nuance. time step often denoted \\(dt\\), must scaled natural time unit, one use proportionality steps calculation. scaling parameter denoted \\(\\tau\\). simulation, often convenient use discrete unit step, instance, (size_t = 0; < nstep; ++). example, took first accumulator, receives large input, 57 steps. 56th step reached 2.7999 activation, slightly lower 2.8 threshold. Without noise term, deterministic accumulator one chosen. \\[dx_i = \\rho_i  \\frac{dt}{\\tau} +  \\xi_i \\sqrt{ \\frac{dt}{\\tau} } \\] following lists basic structure R code implementing equation. coded style, making easier converted C++ readable. figures upper row show C++ implementation results accumulation process R implementation. figures lower row added noise thus, accumulator two sometimes chosen. Without stochastic component, difference drift rates decides accumulator chosen.","code":"dtovertau <- 0.1 dt <- 10  # 10 ms tau <- dt/dtovertau; tau # 100  sdv <- sqrt(dtovertau); sdv  ## Parameters  x0 <- c(0, 0)         # A; starting evidence values  Z  <- 2.8             # b; common threshold across the xi t0 <- .2              # NDT I  <- c(0.5, 0.385)   # inputs from a previous layer = drift rate  maxiter   <- 5001  ## Prepare the process  nacc      <- length(I) output    <- numeric(2) undone    <- TRUE counter   <- 0 random    <- F nonlinear <- F  ## T or F does not matter for this set of parameters  ## initialise accumulator array to record activation value dx <- numeric(nacc);    activation <- x0  at_mat <- matrix(nrow = maxiter, ncol = nacc) dx_mat  <- matrix(nrow = maxiter, ncol = nacc)  at_mat[counter+1,] <- activation dx_mat[counter+1, ] <- 0  repeat {   counter <- counter + 1   for (i in seq_len(nacc)) {          dx[i] <- dtovertau * I[i]     activation[i] <- activation[i] + dx[i]          if(random) activation[i] <- activation[i] + rnorm(1, 0, sdv)          if (activation[i] >= Z) {        output[1] <- counter * dt - 0.5*dt + t0[i];       output[2] <- i;        undone    <- FALSE;        message(\"Activation value: \", activation[i])     }     if (nonlinear && activation[i] < 0) activation[i] <- 0;   }      ac_mat[counter+1, ] <- activation   dx_mat[counter+1,]  <- dx    if (counter > maxiter || !undone) break; }  # counter's first index store step 0 2.8/ (dtovertau*I) counter ## 57"},{"path":"/articles/web_only/4_LCA_v0.html","id":"free-response-paradigm","dir":"Articles > Web_only","previous_headings":"Drift-rate Model","what":"Free Response Paradigm","title":"2. Leaky Competing Accumulation Model","text":"Free response paradigm refers task design permitting decision makers make choice without worrying time limit. , free respond time (interpretation). read term used (Bogacz et al. 2006), 100% sure, standard ubiquitous term. implementing model, design corresponds idea absorbing boundary (Feller 2008), another specialised term, meaning accumulator terminates activity reaches threshold thus, metaphorically absorbed .","code":""},{"path":"/articles/web_only/4_LCA_v0.html","id":"choice-probability","dir":"Articles > Web_only","previous_headings":"Drift-rate Model > Free Response Paradigm","what":"Choice Probability","title":"2. Leaky Competing Accumulation Model","text":"noise, always first accumulator chosen. examined drift-rate difference two accumulators. Fixing drift rate accumulator 1 0.5, differences 0.10 0.13 0.17 0.20 0.23 0.27 0.30 0.33 0.37 0.40. assume accumulator 1 matches target. probability chosen first option larger difference, probable first accumulator chosen. x facet shows drift rate difference subtracting value second accumulator first accumulator. model driven input drift rate, unsurprising see distributions Gaussian shape mean 0 standard deviation sqrt(dtovertau).","code":"# rm(list = ls()) # p.vector <- c(Z = 2.8, t0 = .2, dtovertau = 0.1, dt = 10) # Ilist <- vector(\"list\", length = 10) # I2 <- seq(.4, .1, length.out = 10) # for(i in seq_len(10)) { #      # Imat[i, ] <- c(.5, I2[i])  #        Ilist[[i]] <- c(.5, I2[i])  #      } # fun <- function(I, p.vector, n = 500, nboot = 1000) { #     out <- numeric(nboot) #     for(i in seq_len(nboot)) { #          res <- rLBA(n = n, par = p.vector, I = I, x0 = c(0, 0), 5001, F,  #                                             random = T, debug = F) #              tmp <- table(res@choice_RT[2,]) / n #              out[i] <- tmp[1] #          } #        return(out) # } #  # ncore <- length(Ilist) # out <- parallel::mclapply(Ilist, fun, p.vector, n = 10000, nboot = 1000, #                           mc.cores = ncore)  # save(out, file = \"tests/6_LCA/data/dprob.rda\")  tmp <- matrix(nrow = 10, ncol = 1000) I2 <- seq(.4, .1, length.out = 10) x0 <- NULL for(i in 1:10) {   tmp[i, ] <- out[[i]]   tmp2 <- data.table(x = out[[i]], I = factor(round(I2[i], 2)))   x0 <- rbind(tmp2, x0) }  dprob <- x0  dvline <- data.table(x = rowMeans(tmp), y = 60,                       I =  factor(round(I2, 2)),                       lab = round(rowMeans(tmp), 2)) binsize <- 5 * sd(out[[1]]) / sqrt(length(out[[1]]))  p2 <- ggplot() +   geom_histogram(data = dprob, aes( x = x), binwidth = binsize) +   geom_vline(data = dvline, aes(xintercept = x)) +   geom_text(data = dvline, aes(x = x, y = y, label = lab),               size = 9) +   facet_wrap(I~., scales = \"free_x\") +   theme_bw(base_size = 18) p2 fig <- ggplotly(p2)"},{"path":"/articles/web_only/4_LCA_v0.html","id":"response-time-distributions","dir":"Articles > Web_only","previous_headings":"Drift-rate Model > Free Response Paradigm","what":"Response Time Distributions","title":"2. Leaky Competing Accumulation Model","text":"check numerical details figure, use wonderful plotly package. choice RT distribution using inputs .5 .3 summary, variant model pair reasonable choice RT distributions.","code":"## RT Distribution ----------- plot_hist <- function(x, binwidth = .1, cols = c(\"RT\", \"R\"), print.fig = TRUE) {   upper <- max(x$RT) + 3 * binwidth   bk <- seq(0, upper, by = binwidth)   dtmp <- x[, ..cols]   dA <- dtmp[R == 1, ..cols]   dB <- dtmp[R == 2, ..cols]   rtA <- dA$RT   rtB <- dB$RT   y0 <- cut(rtA, breaks = bk)   y1 <- cut(rtB, breaks = bk)   tmp0 <- cbind(lower = as.numeric( sub(\"\\\\((.+),.*\", \"\\\\1\", y0) ),                 upper = as.numeric( sub(\"[^,]*,([^]]*)\\\\]\", \"\\\\1\", y0) ))   tmp1 <- cbind(lower = as.numeric( sub(\"\\\\((.+),.*\", \"\\\\1\", y1) ),                 upper = as.numeric( sub(\"[^,]*,([^]]*)\\\\]\", \"\\\\1\", y1) ))   dA$mid <- .5*rowSums(tmp0)   dB$mid <- .5*rowSums(tmp1)      d1 <- dA[, .(y = .N / nrow(dtmp)), .(mid)]   d2 <- dB[, .(y = .N / nrow(dtmp)), .(mid)]   names(d1) <- c(\"x\", \"y\")   names(d2) <- c(\"x\", \"y\")   d1$R <- \"1\"   d2$R <- \"2\"   tmp2 <- rbind(d1, d2)      p2 <- ggplot(data = tmp2) +     geom_line( aes(x = x, y = y, colour = R), size = 1) +     geom_point(aes(x = x, y = y, colour = R), size = 2) +     xlab(\"RT (s)\") +  ylab(\"Proportion of trials\") +     theme_minimal(base_size = 20)    if (print.fig) print(p2)   return(tmp2) }  n <- 3000 p.vector <- c(Z = 2.8, t0 = .2, dtovertau = 0.1, dt = 10) res5 <- rLBA(n = n, par = p.vector, I = c(.5, .25), x0 = c(0, 0), 5001, F,               random = T, debug = F) dRT <- data.table(RT = res5@choice_RT[1,]/1000, R =                     factor(res5@choice_RT[2,]))   tmp <- plot_hist(dRT, binwidth = .09)  x0 <- NULL n <- 3000 for(i in 1:1000) {   res5 <- rLBA(n = n, par = p.vector, I = c(.5, .25), x0 = c(0, 0), 5001, F,                 random = T, debug = F)   tmp <- data.table(RT = res5@choice_RT[1,]/1000, R =                        factor(res5@choice_RT[2,]))   tmp$boot <- factor(i)   x0 <- rbind(x0, tmp) }  cols <- c(\"RT\", \"R\") x0[boot == 1, ..cols] x1 <- NULL for(i in seq_len(1e3)) {   tmp <- plot_hist(x0[boot == i, ..cols], binwidth = .09, print.fig = FALSE)   tmp$boot <- factor(i)   x1 <- rbind(x1, tmp) }  save(x0, x1, file = \"tests/6_LCA/data/distribution.rda\") x1$Rboot <- paste0(x1$R, x1$boot) dtmp <- x1[, .(N = .N,        y = mean(y)), .(x, R)] dtmp2 <- dtmp[, c(\"x\", \"y\", \"R\")] dtmp2$Rboot <- paste0(dtmp$R, \"Avg\")  # table(x1$x, x1$R) x1[boot == 1]  p3 <- ggplot(data = x1) +   geom_line( aes(x = x, y = y, group = Rboot, colour = R), size = 1, alpha = .3) +   geom_line(data = dtmp2, aes(x = x, y = y, group = Rboot), size = 1) +   # geom_point(aes(x = x, y = y, colour = R), size = 2) +   xlab(\"RT (s)\") +  ylab(\"Proportion of trials\") +   theme_minimal(base_size = 20)  p3  png(file = \"vignettes/web_only/images/LBA-3.png\", width = 800, height = 600) print(p3) dev.off() fig <- ggplotly(p3) fig"},{"path":"/articles/web_only/4_LCA_v0.html","id":"time-limited-paradigm","dir":"Articles > Web_only","previous_headings":"Drift-rate Model","what":"Time-limited Paradigm","title":"2. Leaky Competing Accumulation Model","text":"Time-limited paradigm refers task design contrast free-response paradigm. free-response paradigm, response (time) window usually set far longer people’s slowest response time time limit . time-limited paradigm, observer given limited amount time make choice. time likely sufficient make high confidence choice many people. modelling process, can implemented large threshold monitor accumulator garnered evidence others time step. probability choosing first accumulator eventually reach 1, given, e.g., 10000 steps. choice RT distribution using inputs .5 .3","code":"## What is the choice probability ? rho <- c(.5, .4) x0 <- c(0, 0) p.vector <- c(Z = 1000, t0 = .2, dtovertau = 0.1, dt = 10) maxiter <- 10000 ncore <- 6  ntrial <- rep(5000, ncore) nlist <- vector(\"list\", length = ncore) for(i in seq_len(ncore)) {  nlist[[i]] <- ntrial[i] } # nlist  fun <- function(ntrial, p.vector, rho, x0, maxiter) {   # cat(ntrial, \"\\n\")   res0 <- rLBA(n = ntrial, par = p.vector, I = rho, x0 = x0,                 maxiter = maxiter, nonlinear = F, random = T, debug = F)    out <- matrix(nrow = maxiter+1, ncol = ntrial)   for(i in seq_len(ntrial)) {     x1 <- res0@activation[1, 1:(maxiter+1) , i]     x2 <- res0@activation[2, 1:(maxiter+1) , i]     out[,i] <- (x1 > x2)   }   return(out) }"},{"path":"/articles/web_only/4_LCA_v0.html","id":"recovery-studies","dir":"Articles > Web_only","previous_headings":"Drift-rate Model","what":"Recovery Studies","title":"2. Leaky Competing Accumulation Model","text":"used generic method, probability density approximation derive model likelihoods used maximum likelihood estimation, instead Bayesian inference, conduct recovery studies. latter also uses likelihood functions, usually uses different method propose candidate parameters estimate estimation intervals. former enough provide answers regarding whether: approximated likelihood function precise enough recover parameters case LCA reduced LCA models. model recoverable range parameters interested . implement process model based previous simplified LCA model (cousin LBA model), called rLBA_internal.","code":"pkg <- c(\"ggplot2\", 'data.table', 'pedxing', 'rbenchmark', 'nloptr', 'plotly') sapply(pkg, require, character.only = T) setwd(\"/media/yslin/Tui/projects/pedxing/\") rm(list = ls()) pal <- Manu::get_pal(\"Kaka\") get_sll <- function(x) { sum( log(x[[1]] ) ) + sum( log(x[[2]] ) ) }"},{"path":"/articles/web_only/4_LCA_v0.html","id":"drift-rate-model-2","dir":"Articles > Web_only","previous_headings":"Drift-rate Model > Recovery Studies","what":"Drift-rate model","title":"2. Leaky Competing Accumulation Model","text":"data set used evaluate recovery simulated data 100 trials. choice-RT data number preliminary model fitting gauge well subplex routine particular one implemented sbplx performs. linking newly written rLBA_internal function spdf routine pda.cpp ggdmc, simulated dLBA function. ideal negative log-likelihood 63. minimisation routine can find parameters, optimisation around data, return negative log-likelihood close value. LCA model non-negative constraint, apply parameters, restricting parameter space. suggested also early LCA paper (Usher McClelland 2001). following shows one example run. proceeding fit model data, conducted parameter survey 2. calculated model likelihoods constructing grid pair drift rates two accumulators. range goes -2 2, using step 0.051. purpose surveying values 0 preliminary tests fitting model simulated data set, found fair percentage (25 %) 100-run bootstrapping test estimated drift rates near 0, given set lower bound 0 parameters knew true values two drift rates 0.5 0.2. result summarised following figure. orange circles shows two discontinuous regions return higher likelihoods purple circles close true values. Two features discontinuous regions stands : one drift rates negative orange circles one drift rates close 0 green circles. Note green circles represent parameters resulting lower likelihoods purple circles, parameters close one resulting maximum likelihood. likely parameter search routine might still end reporting best estimate sampling one green circles two discontinuous regions, jump one low likelihood points near edge concave surface relatively high likelihood point discontinuous regions. PDA method increases likelihood search scenario attempt simplex real maximum likelihood region returns low likelihood noisy likelihood estimation thereby guides search process away maximum likelihood region (see similar problem Bayesian inference (Holmes 2015). summary, resolve problem setting lower bound 0 parameter search routine employ five parallel searches. line theoretical assumption LCA model, race-style accumulation models, assume positive drift rates. employed five parallel searches set zero lower bound parameter search routine resolve problem. Parameter space","code":"p.vector <- c(Z = 2.8, t0 = .2, dtovertau = 0.1, dt = 10/1000) rho <- c(.5, .35) n <- 100 tmp0 <- rLBA_internal(n = n, par = p.vector, I = rho, x0 = c(0, 0), 5001,                        nonlinear = F, random = T)  dat <- data.table(RT = tmp0[1,], R = tmp0[2,])  dat_tmp <- data.table(RT = tmp0[1,], R = tmp0[2,]+1)  png(file = \"vignettes/web_only/images/LBA-5.png\", width = 800, height = 600) plot_hist(dat_tmp, binwidth = .3) dev.off() get_sll <- function(x) { sum( log(x[[1]] ) ) + sum( log(x[[2]] ) ) }  tmp1 <- dLBA(dat$RT, dat$R, nsim = 10000, par = p.vector, I = rho,               x0 = c(0, 0), 5001, F, T, F) -get_sll(tmp1) ##  63.25775 ## dLBA --------------------------- fun <- function (rho, data, debug) {   require(data.table)   # cat(\" \", rho, \"\\n\")   test_rho <<- rbind(test_rho, rho[1:2])   setorder(data, R, RT)   RT <- data$RT   R <- data$R    p.vector <- c(Z = rho[3], t0 = .2, dtovertau = 0.1, dt = 10/1000)    tmp <- dLBA(RT, R, nsim = 10000, par = p.vector, I = rho[1:2], x0 = c(0, 0),                5001, F, T, debug)   out <- -get_sll(tmp)   test_nll <<- c(test_nll, out)   return(out)  }  nmaxit <- 1000 test_rho <- NULL test_nll <- NULL fit <- sbplx(x0 = c(1, 1, 1), fn = fun,               lower = c(0, 0, 0), data = dat, debug = FALSE,                   control=list(maxeval = nmaxit))  dresult <- data.table( true = c(rho, p.vector[1], -get_sll(tmp1)),                         estm = c(fit$par, test_nll[fit$iter]) ) dresult #        true       estm # 1:  0.50000  0.4831009 # 2:  0.35000  0.1935556 # 3:  2.80000  2.5724348 # 4: 63.99076 63.6569213"},{"path":"/articles/web_only/4_LCA_v0.html","id":"start-the-searches","dir":"Articles > Web_only","previous_headings":"Drift-rate Model > Recovery Studies > Drift-rate model","what":"Start the searches","title":"2. Leaky Competing Accumulation Model","text":"Parameter space estimates perfect 2% chance returns parameters resulting high likelihoods, high maximum. Nevertheless, bootstrapped distribution suggest good chance estimate drift rate 1 larger drift rate 2, threshold estimate close true value. sum , now basic LCA formulation. can add leakage term see affects model prediction see can make something can use task manifest signal suppression good hypothesis.","code":"run_fun <- function(i, n) {   # i <- 1   # n <- 100   if(i %% 10 == 0) cat(\"step \", i, \"\\n\")         fun <- function (par, data, debug) {     require(data.table)          test_par <<- rbind(test_par, par)          setorder(data, R, RT)     RT <- data$RT     R <- data$R      p.vector <- c(Z = par[3], t0 = .2, dtovertau = 0.1, dt = 10/1000)          tmp <- dLBA(RT, R, nsim = 10000, par = p.vector, I = par[1:2], x0 = c(0, 0),                 5001, F, T, debug)     out <- -get_sll(tmp)     test_nll <<- c(test_nll, out)     return(out)        }      p.vector <- c(Z = 2.8, t0 = .2, dtovertau = 0.1, dt = 10/1000)   rho <- c(.5, .35)   tmp0 <- rLBA_internal(n = n, par = p.vector, I = rho, x0 = c(0, 0), 5001,                         nonlinear = F, random = T)   dat <- data.table(RT = tmp0[1,], R = tmp0[2,])      npar <- 3      x0 <- runif(npar)   x1 <- runif(npar)   x2 <- runif(npar)   x3 <- runif(npar)   x4 <- runif(npar)   lower <- rep(0, npar)      test_par <- NULL   test_nll <- NULL   fit0 <- sbplx(x0 = x0, fn = fun,                 lower = lower, data = dat, debug = FALSE)   res0 <- list(fit = fit0, parameter = test_par, nll = test_nll, data = dat)    test_par <- NULL   test_nll <- NULL   fit1 <- sbplx(x0 = x1, fn = fun,                  lower = lower, data = dat, debug = FALSE)   res1 <- list(fit = fit1, parameter = test_par, nll = test_nll, data = dat)    test_par <- NULL   test_nll <- NULL   fit2 <- sbplx(x0 = x2, fn = fun,                  lower = lower, data = dat, debug = FALSE)   res2 <- list(fit = fit2, parameter = test_par, nll = test_nll, data = dat)    test_par <- NULL   test_nll <- NULL   fit3 <- sbplx(x0 = x3, fn = fun,                  lower = lower, data = dat, debug = FALSE)   res3 <- list(fit = fit3, parameter = test_par, nll = test_nll, data = dat)    test_par <- NULL   test_nll <- NULL   fit4 <- sbplx(x0 = x4, fn = fun,                  lower = lower, data = dat, debug = FALSE)   res4 <- list(fit = fit4, parameter = test_par, nll = test_nll, data = dat)      fits <- list(fit0, fit1, fit2, fit3, fit4)    idxmin <- which.min(c(fit0$value, fit1$value, fit2$value, fit3$value, fit4$value))   return(fits[[idxmin]]) }  ncore <- 10 t0 <- Sys.time() res <-  parallel::mclapply(1:100, run_fun, n = 100, mc.cores = ncore) t1 <- Sys.time() save(res, file = \"tests/6_LCA/data/test_dLBA_v9.rda\")  true <- c(.5, .35, 2.8) nll <- numeric(100) parameter <- matrix(nrow = 100, ncol = 3) difference <- matrix(nrow = 100, ncol = 3) # i <- 1 for(i in 1:100) {   parameter[i,] <- res[[i]]$par   difference[i,] <- res[[i]]$par - true   nll[i] <- res[[i]]$value      p.vector <- c(Z = 2.8, t0 = .2, dtovertau = 0.1, dt = 10/1000)   p.vector[1] <- res[[i]]$par[3] }  # save(res, parameter, difference, nll, file = \"tests/6_LCA/data/test_dLBA_v9.rda\")  load(\"tests/6_LCA/data/test_dLBA_v9.rda\")  dpar <- data.table(parameter, nll = nll) names(dpar) <- c(\"rho1\", \"rho2\", \"Z\", \"nll_subplex\") round(dpar, 2) dtmp1 <- dpar[nll_subplex > 75.2] dtmp2 <- dpar[nll_subplex <= 75.2]  # The relative standing is kept. No estimates report that rho1 is greater # than rho2, in line with the true values sum( dpar$rho1 > dpar$rho2 ) # 0  # Only two estimates amongst 100 attempts were from the edges of the  # surface  round(dtmp2, 2) #    rho1 rho2    Z nll_subplex # 1: 0.05 0.02 1.16       95.54 # 2: 0.14 0.04 1.16       75.43  nrow(dtmp1)/nrow(dpar) nrow(dtmp2)/nrow(dpar) sum( dtmp2$rho1 > dtmp2$rho2 ) # 98  psych::describe(dtmp1$nll_subplex) #    mean    sd median trimmed   mad   min   max range skew kurtosis    se #  50.93   9.55  51.33   51.06   9.7 30.29 75.12 44.83 -0.02    -0.54 0.96  bw1 <- 3*plotrix::std.error(dtmp2$rho1) bw2 <- 3*plotrix::std.error(dtmp2$rho2) bw3 <- 3*plotrix::std.error(dtmp2$Z) p <- ggplot() +   geom_histogram(data = d1[gp == 1], aes(x = x), binwidth = bw1) +   geom_histogram(data = d1[gp == 2], aes(x = x), binwidth = bw2) +   geom_histogram(data = d1[gp == 3], aes(x = x), binwidth = bw3) +   geom_vline(data = dvline, aes(xintercept = x, color = gp)) +   facet_wrap(.~gp, scales = \"free\")  # png(file = \"vignettes/web_only/images/dLBA-1.png\", width = 800, height = 600) # p # dev.off()"},{"path":"/articles/web_only/4_LCA_v0.html","id":"the-lca-model","dir":"Articles > Web_only","previous_headings":"","what":"The LCA model","title":"2. Leaky Competing Accumulation Model","text":"Figure 1 illustrates two-accumulator LCA structure neural network diagram. Crucially, LCA model carries substantial number parameters. try document details, arrange presentation sequence Equation (4), page 559 use nomenclature (Usher McClelland 2001). parameters variables can divided two categories, main auxiliary. former include investigative mostly researcher-interested parameters/variables. latter sometimes become former, often set constant focus researches due various reasons, simplifying model. \\[ dx_i = (\\rho_i - kx_i - \\beta \\sum_{' \\neq } x_{'} ) \\frac{dt}{\\tau} + \\xi_i \\sqrt{\\frac{dt}{\\tau}} \\] \\[ x_i = max(x_i, 0) \\]","code":""},{"path":"/articles/web_only/4_LCA_v0.html","id":"main-parametersvariables","dir":"Articles > Web_only","previous_headings":"The LCA model","what":"Main Parameters/Variables","title":"2. Leaky Competing Accumulation Model","text":"\\(dx_{}\\) stands increment value neural activity per unit time. subscript \\(\\) index individual accumulator. case two accumulator model, \\(\\) 1 2 0 1, using 0-based programming language (e.g., Python). \\(\\rho_i\\) represents excitatory input previous neural layer. \\(\\kappa\\) means net leakage, \\(\\lambda - \\alpha\\) [eq (1) eq(2), p558]. \\(\\kappa\\) result subtracting rate self-excitatory (.e., \\(\\alpha\\)) decay rate (.e., \\(\\lambda\\)). authors apparently use decay leakage, differently (Usher McClelland 2001). former represents decrease signal adding self-excitation input latter add two together. \\(\\beta\\) weight parameter scale inhibition input accumulators onto ego accumulator. term ego borrowed literature agent-based modelling, meaning self. \\(\\xi_i\\) number sampled Gaussian distribution zero mean variance, \\(\\sigma^2\\). assume \\(\\sigma^2\\) 1, whole term, \\(\\xi_i \\sqrt{\\frac{dt}{\\tau}}\\), can combined common programming routine, instance R, rnorm(), Armadillo C++ randn() (), Python, C++. can check well-tested R routine, use internally. \\(\\frac{dt}{\\tau}\\), see footnote 3, p558. set constant sometimes 0.1 footnote stated, implies corresponds time steps 10 ms. experience scale model times data times time consuming task may become important parameter recovery studies.","code":"## https://pythonexamples.org/python-random-normalvariate/ import random  mu = 2 sigma = 0.5 randomnumber = random.normalvariate(mu, sigma) print(randomnumber) /* https://stackoverflow.com/questions/60721093/random-number-from-normal-distribution-in-c  */   #include <Rcpp.h> #include <random> #include <chrono>  using namespace Rcpp;  // [[Rcpp::export]] double LCA_Gaussian_random(double x) {   unsigned seed = std::chrono::system_clock::now().time_since_epoch().count();   std::default_random_engine generator(seed);   std::normal_distribution<double> distribution(0.0, x);      // Rcout << distribution(generator);   return distribution(generator); } dt <- .001 dtovertau <- 1e-5 tau <- dt/dtovertau; tau sdv <- rep( sqrt(dtovertau), 2); sdv  res1 <- numeric(10000) for(i in seq_len(10000)) {   res1[i] <- pedxing:::LCA_Gaussian_random(sqrt(dtovertau))    } res2 <- rnorm(10000, 0, sqrt(dtovertau)) par(mfrow = c(1,2)) hist(res1) hist(res2) ks.test(res1, res2)  # Asymptotic two-sample Kolmogorov-Smirnov test  #  # data:  res1 and res2  # D = 0.0105, p-value = 0.6399  # alternative hypothesis: two-sided  # No difference both qualitatively and quantitatively rho <- c(1.2, 1)       # inputs; accumulator has a fast rate. kappa <- 3             # (net) leakage rate beta <- 3              # inhibition rate  dt <- .001 dtovertau <- 1e-5 tau <- dt/dtovertau; tau sdv <- rep( sqrt(dtovertau), 2); sdv"},{"path":"/articles/web_only/4_LCA_v0.html","id":"auxiliary-parametersvariables","dir":"Articles > Web_only","previous_headings":"The LCA model","what":"Auxiliary Parameters/Variables","title":"2. Leaky Competing Accumulation Model","text":"Additional variables / parameters shown equation (4, p559) include: \\(x_0\\), starting activity accumulators, \\(t_0\\), non-decision time. \\(Z\\), response threshold, usually set commonly accumulators. nonLinear Boolean, indicating whether apply sub-equation, \\(x_i = max(x_i, 0)\\). , accumulated activity becomes negative, reset 0. assume neural activity negative. variable, maxiter, programming method limit maximum time step -loop steps time. re-organising many processing steps function class C++, following R code steps step. code take lot hints Miletić’s (Miletić et al. 2017) Github repository. download original code Internet search, still searchable open source time read vignette. First initialise parameters number accumulator several containers record process. variable, winner, used process, indicating whether accumulators reached threshold, process stopped. counter keeps track time step. counter start 1; however, R indexing, 2, R object counts 1, instead 0. -loop simulate process time. random flag allows investigate deterministic process adding stochastic component. outcome deterministic, two-accumulator LCA process completely determined starting value, \\(x0 = c(.01, .02)\\), excitatory inputs, \\(= c(1.2, 1)\\), fixed threshold, \\(Z = 0.2\\) Deterministic LCA earliest ERP, namely Lateralization Readiness Potential(LRP), probably match lateral inhibition process (Gratton et al. 1988).","code":"# starting evidence values; accumulator has less starting activity x0 <- c(.01, .02)  t0 <- .3          # NDT Z <- .2           # common threshold across accumulators  maxiter <- 5001  sdv <- sqrt(dtovertau)   # common sdv sdv^2 is sigma^2  xi  <- rnorm(maxiter, 0, sdv); output <- numeric(2)   ## RT and choice nacc   <- length(I)  beta  <- rep(beta, nacc) Z     <- rep(Z, nacc) kappa <- rep(kappa, nacc) t0    <- rep(t0, nacc) winner  <- F counter <- 1;  leak <- activation <- inhibition <- numeric(nacc);    for (i in seq_len(nacc)) {        activation[i] <- x0[i]        inhibition[i] <- 0       leak[i] <- 0 }  act_mat <- leak_mat <- inhibition_mat <- matrix(nrow = maxiter, ncol = nacc) act_mat[counter,] <- activation leak_mat[counter,] <- 0 inhibition_mat[counter,] <- 0 nonLinear <- TRUE random <- FALSE ## Start process repeat {       counter <- counter + 1        # A simple mathematical trick to get total the other inhibition value       # without doing a complex indexing scheme.       total_inhibition = 0;       for (i in seq_len(nacc))       {         inhibition[i]    <- dt * beta[i]*activation[i];           leak[i]          <- dt * kappa[i]*activation[i]         total_inhibition <- total_inhibition + inhibition[i];       }              for (i in seq_len(nacc))       {         inhibition_from_others <- total_inhibition - inhibition[i]         dx <- dt*I[i] - leak[i] - inhibition_from_others # dx_i         activation[i] <- activation[i] + dx ## x_i          if (random) activation[i] <- activation[i] + rnorm(1, 0, sdv[i])         if (activation[i] >= Z[i]) {            output[1] <- (counter+1) * dt - 0.5*dt + t0[i];           output[2] <- i;            winner    <- TRUE;          }         ## If simulating a non-linear LCA and accumulators with values lower          ## than 0, reset it to 0.         if (nonLinear && activation[i] < 0) activation[i] <- 0;       }              act_mat[counter, ]        <- activation         inhibition_mat[counter, ] <- inhibition       leak_mat[counter, ]       <- leak        if (counter > maxiter | winner) break; }    # 0.5125 1.0000"},{"path":"/articles/web_only/4_LCA_v0.html","id":"list-of-equations","dir":"Articles > Web_only","previous_headings":"","what":"List of Equations","title":"2. Leaky Competing Accumulation Model","text":"quote also words describing symbols, equations, assumptions.","code":""},{"path":"/articles/web_only/4_LCA_v0.html","id":"equation-1","dir":"Articles > Web_only","previous_headings":"List of Equations","what":"Equation (1)","title":"2. Leaky Competing Accumulation Model","text":"Found page 558. \\[ dx_i = [I_i - \\lambda x_i] \\frac{dt}{\\tau} + \\xi \\sqrt{ \\frac{dt}{\\tau} } \\] \\(\\tau\\) time scale chosen convenience, \\(\\xi_i\\) Gaussian noise term zero mean variance \\(\\sigma^2\\). equation implies within time interval, \\(dt/\\tau\\), change activation accumulator unit, \\(dx_i\\), driven input unit, \\(I_i\\) previous layer. input results activation accumulator layer. activation accumulator decays characteristic decay rate, defined model parameters \\(\\lambda\\). non-stochastic version model, noise term scales square root \\(dt/\\tau\\), variance uncorrelated stochastic random variables additive, leading square-root behaviour standard deviation. Assuming \\(\\tau=1\\) noise, can rewrite equation \\[ dx_i = [I_i - \\lambda x_i] dt} \\] authors stated one assumes \\(I_i\\) can suddenly change 0 fixed value, equation produces exponential approach leads \\(\\frac{dx_i}{dt} = 0\\). rearranging equation, one can get \\(x^{asy}_i = I_i / \\lambda\\). explanation asy refers . Perhaps, one imporntant information note “fixed input \\(I_i\\)”. , excitatory input seems fixed value time. \\(I_i = ^{ext}_i + ^{rec}_i + ^{LI}_{j's}\\) \\(^{rec}_i = \\alpha f_i\\) \\(f_i\\) output ith accumulator? assumption \\(^{ext}_i\\) 0 presentation (imperative) stimulus. “Likewise accumulator units initialized 0 stimulus onset except …” Output input units set values \\(f_j{(\\Phi)}\\) presentation stimulus \\(\\Phi\\). \\(f_j\\) assumed $ $. \\(^{ext}_i\\) posited weighted sum output inputs units, \\(^{ext}_i = \\sum_j W_{ij} f_j\\). \\(j\\) means several inputs previous layer. \\(\\rho = ^{ext}_i(\\Phi)\\).","code":""},{"path":"/articles/web_only/4_LCA_v0.html","id":"equation-2","dir":"Articles > Web_only","previous_headings":"List of Equations","what":"Equation (2)","title":"2. Leaky Competing Accumulation Model","text":"second equation introduces inhibitory influences exerted accumulators ego accumulator. authors first stated inhibitory input others eqo accumulator weighted sum outputs others accumulators, assumed simplified assumption weight units , denoted \\(\\beta\\). elegant way write equation \\(\\beta \\sum_{' \\neq } f_{'}\\). exactly implement computer languages, unfortunately. equation (2) finally becomes: \\[ dx_i = ( \\rho + \\alpha f_i - \\beta \\sum_{' \\neq } f_{'} - \\lambda x_i ) \\frac{dt}{\\tau} +  \\xi_i \\sqrt{ \\frac{dt}{\\tau} } \\] ⚠\\(\\rho\\) equation (2) equal \\(I_i\\) equation (1)⚠. \\(\\rho\\) represents part \\(I_i\\). , \\(^{ext}_i\\). Term 1 (\\(\\rho\\)), term 2 (\\(\\alpha f_i\\)), term 3 (\\(\\beta \\sum_{' \\neq } f_{'}\\)), together, represented \\(I_i\\) equation (1). \\(\\lambda x_i\\) represents leakage activation. confusing authors stated equation (2) non-linear, later sentences , stated “note equation ( probably refer equation (2), closest one ) completely linear long \\(x_i\\) greater 0.” interpretation considering without stochastic term, linear equation uses addition multiplication. Although one might argue inhibition term makes non-linear, involves element re-organization, like procedure convolution.","code":""},{"path":"/articles/web_only/4_LCA_v0.html","id":"equation-3","dir":"Articles > Web_only","previous_headings":"List of Equations","what":"Equation (3)","title":"2. Leaky Competing Accumulation Model","text":"assuming accumulated activation accumulators, \\(x_i > 0\\), authors argued one can replace \\(f_i\\) \\(x_i\\), rendering equation (2) equation (3): \\[ dx_i = [ \\rho - (\\lambda - \\alpha) x_i - \\beta \\sum_{' \\neq } f_{'} ] \\frac{dt}{\\tau} +  \\xi_i \\sqrt{ \\frac{dt}{\\tau} } \\] renders new term called net leakage (rate), \\(\\kappa = \\lambda - \\alpha\\). seems interesting, results neuronal plausible assumption \\(\\kappa > 0\\), net effect decay toward 0 produces stability (equilibrium?); less 0, activation tends self-amplify stable (action potential?).","code":""},{"path":"/articles/web_only/4_LCA_v0.html","id":"equation-4","dir":"Articles > Web_only","previous_headings":"List of Equations","what":"Equation (4)","title":"2. Leaky Competing Accumulation Model","text":"\\[ dx_i = [ \\rho - \\kappa x_i - \\beta \\sum_{' \\neq } x_{'} ] \\frac{dt}{\\tau} +  \\xi_i \\sqrt{ \\frac{dt}{\\tau} } \\] \\(x_i = max(x_i, 0)\\)","code":""},{"path":"/articles/web_only/4_LCA_v0.html","id":"usher-and-mcclellends-appendix-a","dir":"Articles > Web_only","previous_headings":"","what":"Usher and McClellend’s Appendix A","title":"2. Leaky Competing Accumulation Model","text":"First quoting footnote 3, > time scale tau can ignored one uses natural time scale > seconds milliseconds. However, integrating equations numerically, > convenient use time scale one order magnitude > temporal process observed ensure time step, dt, > smaller tau, required Euler integration3. > value dt/tau = 0.1 used numerical integrations, > corresponding time steps 10 ms.","code":""},{"path":"/articles/web_only/4_LCA_v0.html","id":"equation-6","dir":"Articles > Web_only","previous_headings":"Usher and McClellend’s Appendix A","what":"Equation (6)","title":"2. Leaky Competing Accumulation Model","text":"Assuming \\(\\rho_1 + \\rho_2 = 1\\) reducing case 2AFC time-controlled conditions, equation becomes: amount time allowed processing controlled, response made whenever \\(x_1-x_2 > B\\), criterion. \\[ dx = [ (2 \\rho_1 - 1) - (\\kappa - \\beta) x ] \\frac{dt}{\\tau} +  \\xi \\sqrt{ \\frac{dt}{\\tau} } \\] process described Equation 6 OU process (Ricciardi, 1977?). \\(K = \\kappa - \\beta\\). \\(\\kappa\\) specifically means unitwise leakage, incorporate inhibitory influences units. \\(K\\) provides measure net leakage difference two units’ activation. mean \\(K\\) applies case two units? decision-field theory used sign \\(K\\) coefficient rates approach-avoidance characteristics decision (monetary risk decision) (Busemeyer Townsend 1993). negative \\(K\\) OU process used time-inhomogeneous model studying simple reaction times luminance-increment detection (Smith 1995). define model parameters. vlaues found text figures Appendix. exact numerical values regarding inputs provided, made wild guesses based figure. Trimming NA’s containers. Deterministic LCA 2","code":"dtovertau <- 0.1 dt <- 10  # 10 ms tau <- dt/dtovertau; tau # 100 ms sdv <- sqrt(dtovertau); sdv lambda <- 8.7 alpha <- 8.5 K <- lambda - alpha  ## kappa? x0 <- c(0, 0) # starting evidence values for the layer where accumulators are  beta  <- .4    # inhibition rate Z  <- 2.8      # common threshold across the 3 acc's t0 <- .2      # NDT  I  <- c(0.5, 0.485)   # inputs from a previous layer maxiter   <- 5001  ## Prepare the process --------- nacc <- length(I) output <- numeric(2) undone  <- TRUE counter <- 0; random <- F nonLinear <- F  ## T or F does not matter for this set of parameters   ##  initialize accumulator array to record activation value activation <- inhibition <- leak <- inhibit_received <- dx <- numeric(nacc);    for (i in seq_len(nacc)) {   activation[i] <- x0[i];   inhibition[i] <- 0;   leak[i] <- 0   inhibit_received[i] <- 0   dx[i] <- 0 }   act_mat <- leak_mat <- inhibition_mat <- matrix(nrow = maxiter, ncol = nacc) inh_rece_mat <- dx_mat <- matrix(nrow = maxiter, ncol = nacc)  act_mat[counter+1,] <- activation leak_mat[counter+1,] <- 0 inhibition_mat[counter+1,] <- 0 inh_rece_mat[counter+1, ] <- 0 dx_mat[counter+1, ] <- 0   repeat {   counter <- counter + 1   if (counter > maxiter) break;      total_inhibition <- 0   for (i in seq_len(nacc)) {     inhibition[i] <- dtovertau * beta * activation[i];       leak[i] <- dtovertau * K * activation[i]     total_inhibition <- total_inhibition + inhibition[i];   }    for (i in seq_len(nacc)) {     inhibit_received[i] <- (total_inhibition - inhibition[i])     dx[i] <- dtovertau * I[i] - leak[i] - inhibit_received[i]      activation[i] <- activation[i] + dx[i]      if(random) activation[i] <- activation[i] + rnorm(1, 0, sdv)      if (activation[i] >= Z) {        output[1] <- counter * dt - 0.5*dt + t0[i];       output[2] <- i;        undone    <- FALSE;        message(\"Activity \", activation[i])     }     if (nonLinear && activation[i] < 0) activation[i] <- 0;   }    act_mat[counter+1, ]        <- activation   dx_mat[counter+1,]          <- dx   inhibition_mat[counter+1, ] <- inhibition   leak_mat[counter+1, ]       <- leak   inh_rece_mat[counter+1, ]   <- inhibit_received   if (counter > maxiter | !undone) break; } for(j in seq_len(maxiter)) {   if ( all(is.na(act_mat[j,])) ) { idx <- j-1; break } }  head(act_mat[1:(idx+1),]) tail(act_mat[1:(idx+1),]) counter idx Z  out <- list(CRT = output, trace = act_mat[1:idx,],             leakage = leak_mat[1:idx,],              lateral_inhibition = inh_rece_mat[1:idx,],             lateral_inhibition2 = inhibition_mat[1:idx,],             dx = dx_mat[1:idx,]             ) pal <- Manu::get_pal(\"Kaka\") leg.txt <- c(\"Acc1\", \"Acc2\") # par(pty = \"s\", mfrow = c(2, 2))  par(pty = \"s\") yl <- range(out$trace) plot(1:idx, out$trace[,1], pch = 1,       ylim = c(-3, 3), col = pal[1],       # lwd = 3, type = \"l\",       xlab = \"time step\", ylab = \"Activity\") points(1:idx, out$trace[,2], col = pal[2], pch = 2)        # lwd = 3) abline(h = Z[1], lty = \"dotted\") abline(h = 0, lty = \"dotted\") # par(pty = \"s\", mfrow = c(1, 1))  legend(\"topleft\", leg.txt, col = pal[1:2], pch = c(1, 2))"},{"path":"/articles/web_only/4_LCA_v0.html","id":"programming-note","dir":"Articles > Web_only","previous_headings":"","what":"Programming Note","title":"2. Leaky Competing Accumulation Model","text":"used five packages support code vignette. pedxing customised package associated manuscript work. going hosted personal Github.","code":"pkg <- c(\"ggplot2\", 'data.table', 'pedxing', 'rbenchmark', 'plotly') sapply(pkg, require, character.only = T) setwd(\"/media/yslin/Tui/projects/pedxing/\") rm(list = ls())"},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Yi-Shin Lin. Maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lin Y (2022). pedxing: Crossing decisions. R package version 0.0.0.2.","code":"@Manual{,   title = {pedxing: Crossing decisions},   author = {Yi-Shin Lin},   year = {2022},   note = {R package version 0.0.0.2}, }"},{"path":"/reference/Inhibition-class.html","id":null,"dir":"Reference","previous_headings":"","what":"An S4 class for the Inhibition model — Inhibition-class","title":"An S4 class for the Inhibition model — Inhibition-class","text":"S4 class Inhibition model","code":""},{"path":"/reference/Inhibition-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"An S4 class for the Inhibition model — Inhibition-class","text":"choice_RT choice_RT activation activation dx dx leak leakage counter counter samples simulation samples","code":""},{"path":"/reference/Input-class.html","id":null,"dir":"Reference","previous_headings":"","what":"An S4 class for the Input-only model — Input-class","title":"An S4 class for the Input-only model — Input-class","text":"S4 class Input-model","code":""},{"path":"/reference/Input-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"An S4 class for the Input-only model — Input-class","text":"choice_RT choice_RT activation activation dx dx counter counter","code":""},{"path":"/reference/LCA-class.html","id":null,"dir":"Reference","previous_headings":"","what":"An S4 class — LCA-class","title":"An S4 class — LCA-class","text":"S4 class","code":""},{"path":"/reference/LCA-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"An S4 class — LCA-class","text":"choice_RT choice_RT activation activation inhibition inhibition leakage leakage dx dx counter counter","code":""},{"path":"/reference/Leak-class.html","id":null,"dir":"Reference","previous_headings":"","what":"An S4 class for the Leakage model — Leak-class","title":"An S4 class for the Leakage model — Leak-class","text":"S4 class Leakage model","code":""},{"path":"/reference/Leak-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"An S4 class for the Leakage model — Leak-class","text":"choice_RT choice_RT activation activation dx dx leak leakage counter counter samples simulation samples","code":""},{"path":"/reference/foresee_next.html","id":null,"dir":"Reference","previous_headings":"","what":"Foresee a next state — foresee_next","title":"Foresee a next state — foresee_next","text":"Get predictive next position pedestrian (R function used  testing. Another copy C++ function runs model fitting.)","code":""},{"path":"/reference/foresee_next.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Foresee a next state — foresee_next","text":"","code":"foresee_next(x0, v, pt, t0 = 0)"},{"path":"/reference/foresee_next.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Foresee a next state — foresee_next","text":"x0 initial (pedestrian) position v pedestrian walking speed pt far time pedestrian can foresee consequence  action may result t0 much time needed enact action; assuming instantaneous t0 = 0; map t0.","code":""},{"path":"/reference/foresee_next.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Foresee a next state — foresee_next","text":"Function get predictive pedestrian state beginning cross.  assume pedestrian prepares walk north negative side y axis.","code":""},{"path":"/reference/foresee_next.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Foresee a next state — foresee_next","text":"","code":"## some examples"},{"path":"/reference/get_traj.html","id":null,"dir":"Reference","previous_headings":"","what":"Get trajectory data out of a rModel function — get_traj","title":"Get trajectory data out of a rModel function — get_traj","text":"Get trajectory data rModel function","code":""},{"path":"/reference/get_traj.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get trajectory data out of a rModel function — get_traj","text":"","code":"get_traj(x, model = \"inhibition\")"},{"path":"/reference/get_traj.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get trajectory data out of a rModel function — get_traj","text":"x model output model string identify model","code":""},{"path":"/reference/get_traj.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get trajectory data out of a rModel function — get_traj","text":"","code":"## some examples"},{"path":"/reference/p.html","id":null,"dir":"Reference","previous_headings":"","what":"The Probability of Moving Up or Down (Method 1) — p","title":"The Probability of Moving Up or Down (Method 1) — p","text":"p gives probability (p) moving one unit evidence (ie dx) .  Setting is_minus TRUE results probability 1 - p,  probability moving one unit. get_dx returns  displacement one evidence unit. functions implement  equation (6) page 445.","code":""},{"path":"/reference/p.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The Probability of Moving Up or Down (Method 1) — p","text":"","code":"p(v, s = 1, h = 0.001, is_minus = FALSE)  get_dx(s, h)"},{"path":"/reference/p.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Probability of Moving Up or Down (Method 1) — p","text":"v drift rate s standard deviation diffusion process. Note variance process s^2. h size  time step is_minus TRUE, return q function; otherwise, return p function.","code":""},{"path":"/reference/p.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The Probability of Moving Up or Down (Method 1) — p","text":"probability value. vectorization depends R's default behaviour.","code":""},{"path":"/reference/p.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"The Probability of Moving Up or Down (Method 1) — p","text":"Tuerlinckx, F., Maris, E., Ratcliff, R., & De Boeck, P. (2001).  comparison four methods simulating diffusion process.  Behavior Research Methods, Instruments, & Computers, 33(4), 443-456.","code":""},{"path":"/reference/p.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The Probability of Moving Up or Down (Method 1) — p","text":"","code":"mu <- 1.2 sigma <- 1 h <- .001        ## time step dx <- get_dx(sigma, h)  n <- 1000      prob_up <- p(v=mu, s=sigma, h=h)   prob_down <- p(v=mu, s=sigma, h=h, is_minus = TRUE)    y <- sample(x = c(dx, -dx), size = n, replace = TRUE,              prob = c(prob_up, prob_down)) plot(1:n, cumsum(y), type = \"l\", xlab = \"DT (ms)\", ylab = \"Evidence\",      main = \"A Random Walk Approximation\") points(0, 0, col = \"red\")"},{"path":"/reference/ped.html","id":null,"dir":"Reference","previous_headings":"","what":"Crossing decisions data — ped","title":"Crossing decisions data — ped","text":"ped stores three data frames, d, d0  dat. snew sold two labelling schemes  participant labels. multiple data frames enable different  software process conveniently. d0 variables  aggregated across participants. dat reduces columns. d processed RStan via rethinking package. Trial. label recording also sequence trial appearing task. Jitter. randomly selected time -0.1 0.1 s. added    small variability time-##' -arrival prevent responses    based anticipating imperative stimulus. BeforeCarPassed. Boolean variable, indicating response made car   passes crossing zone. E. nominal variable, indicating whether trial behavioural   study, exp, EEG study, eeg. TTA. Time--arrival second. 2.5, 3, 3.5 4 seconds. Side. nominal variable, indicating trial showing avator,   beginning, stands right left side scene. R. nominal variable, indicating trial resulting safe response collision. C. Boolean variable, indicating trial resulting safe response (TRUE) collision (FALSE). G. nominal variable, indicating response made car passes crossing zone. TTANme. nominal variable time--arrival. RT. continuous variable, indicating response times second. s. nominal variable, indicating different participants. D. continuous variable vehicle initial distance. column   one value, .e., 40 m. calculate vehicle kinematics. DTTA. nominal label distance TTA purpose plotting figures.","code":""},{"path":"/reference/ped.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Crossing decisions data — ped","text":"","code":"data(ped)"},{"path":"/reference/ped.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Crossing decisions data — ped","text":"Behavioural data human participants road-crossing task.","code":""},{"path":"/reference/ped.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Crossing decisions data — ped","text":"Institute Transport Studies, University Leeds. work supported   Engineering Physical Sciences Research Council (EPSRC) Grant   number, EP/S005056/1.","code":""},{"path":"/reference/ped.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Crossing decisions data — ped","text":"Yi-Shin Lin <yishinlin@pm.>","code":""},{"path":"/reference/ped.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Crossing decisions data — ped","text":"","code":"data(ped) dplt <- dat[RT <= 6 & RT > .1] 1 - nrow(dplt) / nrow(dat) #> [1] 0.002103365   # Simple get_break function rt <- dplt$RT binsize <- psych::describe(rt)$se   bk <- seq(min(rt)-binsize, max(rt)+binsize, by = binsize)  dv <- data.frame(TTA = c(2.5, 3, 3.5, 4, 2.5, 3, 3.5, 4),                  Etta = c(\"exp, 2.5\", \"exp, 3\", \"exp, 3.5\", \"exp, 4\",                           \"eeg, 2.5\", \"eeg, 3\", \"eeg, 3.5\", \"eeg, 4\"))  # sort(unique(DT$TTA)) # [1] 2.5 3.0 3.5 4.0   dplt$Rgp <- factor( paste0(dplt$G, \", \", dplt$R),                      levels = c(\"after, safe\", \"after, hit\", \"before, safe\", \"before, hit\")) table(dplt$Rgp) #>  #>  after, safe   after, hit before, safe  before, hit  #>         4269            7         4577         1110  if (FALSE) {  p0 <- ggplot() +   geom_histogram(data = dplt[E==\"exp\"], aes(x = RT, fill = Rgp),                   breaks = bk) +   geom_histogram(data = dplt[E==\"eeg\"], aes(x = RT, fill = Rgp),                   breaks = bk) +   ## scale_fill_manual(values = Manu::get_pal(\"Tui\")) +   geom_vline(data = dv, aes(xintercept = TTA), linetype = \"dashed\") +   scale_x_continuous(name = \"RT (s)\", breaks = c(0, 1, 2, 3, 4, 5, 6)) +   facet_grid(E~., switch = \"y\") +   theme_minimal(base_size = 16) +   theme(legend.position = \"none\",         # theme(legend.position = c(.80, .90),         legend.title = element_blank(),         strip.placement = \"outside\")  dplt$Etta <- factor( paste0(dplt$E, \", \", dplt$TTA),                       levels = c(\"exp, 2.5\", \"exp, 3\", \"exp, 3.5\", \"exp, 4\",                                 \"eeg, 2.5\", \"eeg, 3\", \"eeg, 3.5\", \"eeg, 4\"))  p1 <- ggplot() +   geom_histogram(data = dplt[E==\"exp\"], aes(x = RT, fill = Rgp),                   breaks = bk) +   geom_histogram(data = dplt[E==\"eeg\"], aes(x = RT, fill = Rgp),                   breaks = bk) +   ## scale_fill_manual(values = Manu::get_pal(\"Tui\")) +   geom_vline(data = dv, aes(xintercept = TTA), linetype = \"dashed\") +   scale_x_continuous(name = \"RT (s)\", breaks = c(0, 1, 2, 3, 4, 5, 6)) +   ylab(\"\") +   facet_grid(Etta~., switch = \"y\") +   theme_minimal(base_size = 16) +   theme(legend.position = c(.85, .95),         legend.title = element_blank(),         strip.placement = \"outside\")  gridExtra::grid.arrange(p0, p1, ncol = 2) }"},{"path":"/reference/pedxing.html","id":null,"dir":"Reference","previous_headings":"","what":"Crossing decisions — pedxing","title":"Crossing decisions — pedxing","text":"Version history:","code":""},{"path":"/reference/pedxing.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Crossing decisions — pedxing","text":"Yi-Shin Lin <yishinlin@pm.>","code":""},{"path":"/reference/plot_hist.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot choice RT distributions — plot_hist","title":"Plot choice RT distributions — plot_hist","text":"Plot choice RT distributions","code":""},{"path":"/reference/plot_hist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot choice RT distributions — plot_hist","text":"","code":"plot_hist(x, binwidth = 0.1, cols = c(\"RT\", \"R\"), print.fig = TRUE)"},{"path":"/reference/plot_hist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot choice RT distributions — plot_hist","text":"x data frame binwidth bin width cols key columns print.fig Boolean","code":""},{"path":"/reference/plot_hist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot choice RT distributions — plot_hist","text":"","code":"## some examples"},{"path":"/reference/rInput_R.html","id":null,"dir":"Reference","previous_headings":"","what":"LBA Process in an R function — rInput_R","title":"LBA Process in an R function — rInput_R","text":"Reconstruct LBA LCA structure","code":""},{"path":"/reference/rInput_R.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"LBA Process in an R function — rInput_R","text":"","code":"rInput_R(   n,   par,   I,   x0,   maxiter = 5001,   nonlinear = FALSE,   random = FALSE,   debug = FALSE )"},{"path":"/reference/rInput_R.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"LBA Process in an R function — rInput_R","text":"n number trials par par x0 x0 maxiter maxiter nonlinear nonlinear random random debug debug","code":""},{"path":"/reference/rInput_R.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"LBA Process in an R function — rInput_R","text":"","code":"## some examples"},{"path":"/reference/rLCA_R.html","id":null,"dir":"Reference","previous_headings":"","what":"LCA Process in an R function — rLCA_R","title":"LCA Process in an R function — rLCA_R","text":"LCA Process R function","code":""},{"path":"/reference/rLCA_R.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"LCA Process in an R function — rLCA_R","text":"","code":"rLCA_R(   n,   par,   I,   x0,   maxiter = 5001,   nonlinear = FALSE,   random = FALSE,   debug = FALSE )"},{"path":"/reference/rLCA_R.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"LCA Process in an R function — rLCA_R","text":"n number trials par par x0 x0 maxiter maxiter nonlinear nonlinear random random debug debug","code":""},{"path":"/reference/rLCA_R.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"LCA Process in an R function — rLCA_R","text":"","code":"p.vector <- c(kappa = 3, beta = 3, Z = .2, t0 = .3, dtovertau = 0.0001, tau = 10) args(rLCA_R) #> function (n, par, I, x0, maxiter = 5001, nonlinear = FALSE, random = FALSE,  #>     debug = FALSE)  #> NULL res <- rLCA_R(n = 1, par = p.vector, I = c(1.2, 1), x0 = c(0, 0))  nacc <- length(I) d1 <- data.table(x = rep(1:nrow(res[[1]]$trace), nacc),               y1 = as.vector %>% r(res[[1]]$trace),               y2 = as.vector(res[[1]]$lateral_inhibition),              y3 = as.vector(res[[1]]$leakage),              y4 = as.vector(res[[1]]$dx),              z = factor(rep(1:nacc, each = nrow(res[[1]]$trace)))) #> Error in data.table(x = rep(1:nrow(res[[1]]$trace), nacc), y1 = as.vector %>%     r(res[[1]]$trace), y2 = as.vector(res[[1]]$lateral_inhibition),     y3 = as.vector(res[[1]]$leakage), y4 = as.vector(res[[1]]$dx),     z = factor(rep(1:nacc, each = nrow(res[[1]]$trace)))): could not find function \"data.table\"   p1 <- ggplot() + geom_line(data = d1, aes(x=x, y=y1, colour =z)) +   geom_hline(yintercept = Z[1]) + xlab(\"Time step\") + ylab(\"Activation\") + theme_bw(base_size = 20) + theme(aspect.ratio=1) #> Error in ggplot(): could not find function \"ggplot\" p1 #> Error in eval(expr, envir, enclos): object 'p1' not found"}]
